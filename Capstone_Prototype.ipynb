{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import Input, Dense, Reshape, Dropout, Activation\n",
    "from keras.layers import LSTM, Bidirectional, BatchNormalization, ZeroPadding2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "import pretty_midi\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plot\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "import pandas_profiling\n",
    "\n",
    "#Path for the groove dataset\n",
    "MIDI_PATH = \"/home/mark/repos/Springboard/data/\"\n",
    "DATA_PATH = \"/home/mark/repos/Springboard/data/info.csv\"\n",
    "\n",
    "groove_df = pd.read_csv(DATA_PATH)\n",
    "#groove_df"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---------------------- \n",
    "    DATA WRANGLING\n",
    "---------------------- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jazz/swing\n",
      "['jazz', 'swing']\n",
      "jazz\n",
      "neworleans/funk\n",
      "['neworleans', 'funk']\n",
      "funk\n",
      "latin/brazilian-samba\n",
      "['latin', 'brazilian-samba']\n",
      "latin\n",
      "blues/shuffle\n",
      "['blues', 'shuffle']\n",
      "hiphop/groove6\n",
      "['hiphop', 'groove6']\n",
      "hiphop\n",
      "jazz/fusion\n",
      "['jazz', 'fusion']\n",
      "jazz\n",
      "soul/groove9\n",
      "['soul', 'groove9']\n",
      "soul\n",
      "funk/groove5\n",
      "['funk', 'groove5']\n",
      "funk\n",
      "soul/groove3\n",
      "['soul', 'groove3']\n",
      "soul\n",
      "latin/brazilian\n",
      "['latin', 'brazilian']\n",
      "latin\n",
      "funk/groove2\n",
      "['funk', 'groove2']\n",
      "funk\n",
      "pop/groove7\n",
      "['pop', 'groove7']\n",
      "pop\n",
      "soul/groove4\n",
      "['soul', 'groove4']\n",
      "soul\n",
      "soul/motown\n",
      "['soul', 'motown']\n",
      "soul\n",
      "rock/groove8\n",
      "['rock', 'groove8']\n",
      "rock\n",
      "funk/groove1\n",
      "['funk', 'groove1']\n",
      "funk\n",
      "soul/groove10\n",
      "['soul', 'groove10']\n",
      "soul\n",
      "neworleans/secondline\n",
      "['neworleans', 'secondline']\n",
      "latin/brazilian-songo\n",
      "['latin', 'brazilian-songo']\n",
      "latin\n",
      "latin/brazilian-baiao\n",
      "['latin', 'brazilian-baiao']\n",
      "latin\n",
      "latin/brazilian-ijexa\n",
      "['latin', 'brazilian-ijexa']\n",
      "latin\n",
      "pop/soft\n",
      "['pop', 'soft']\n",
      "pop\n",
      "latin/brazilian-sambareggae\n",
      "['latin', 'brazilian-sambareggae']\n",
      "latin\n",
      "dance/disco\n",
      "['dance', 'disco']\n",
      "latin/chacarera\n",
      "['latin', 'chacarera']\n",
      "latin\n",
      "jazz/march\n",
      "['jazz', 'march']\n",
      "jazz\n",
      "rock/shuffle\n",
      "['rock', 'shuffle']\n",
      "rock\n",
      "rock/prog\n",
      "['rock', 'prog']\n",
      "rock\n",
      "rock/indie\n",
      "['rock', 'indie']\n",
      "rock\n",
      "funk/latin\n",
      "['funk', 'latin']\n",
      "funk\n",
      "latin\n",
      "latin/brazilian-bossa\n",
      "['latin', 'brazilian-bossa']\n",
      "latin\n",
      "afrocuban/calypso\n",
      "['afrocuban', 'calypso']\n",
      "afrocuban\n",
      "jazz/mediumfast\n",
      "['jazz', 'mediumfast']\n",
      "jazz\n",
      "latin/brazilian-maracatu\n",
      "['latin', 'brazilian-maracatu']\n",
      "latin\n",
      "jazz/linear\n",
      "['jazz', 'linear']\n",
      "jazz\n",
      "latin/samba\n",
      "['latin', 'samba']\n",
      "latin\n",
      "jazz/funk\n",
      "['jazz', 'funk']\n",
      "jazz\n",
      "funk\n",
      "dance/breakbeat\n",
      "['dance', 'breakbeat']\n",
      "funk/fast\n",
      "['funk', 'fast']\n",
      "funk\n",
      "latin/ando\n",
      "['latin', 'ando']\n",
      "latin\n",
      "jazz/klezmer\n",
      "['jazz', 'klezmer']\n",
      "jazz\n",
      "rock/folk\n",
      "['rock', 'folk']\n",
      "rock\n",
      "rock/rockabilly\n",
      "['rock', 'rockabilly']\n",
      "rock\n",
      "latin/venezuelan-sangueo\n",
      "['latin', 'venezuelan-sangueo']\n",
      "latin\n",
      "jazz/fast\n",
      "['jazz', 'fast']\n",
      "jazz\n",
      "latin/venezuelan-merengue\n",
      "['latin', 'venezuelan-merengue']\n",
      "latin\n",
      "neworleans/shuffle\n",
      "['neworleans', 'shuffle']\n",
      "latin/brazilian-frevo\n",
      "['latin', 'brazilian-frevo']\n",
      "latin\n",
      "rock/halftime\n",
      "['rock', 'halftime']\n",
      "rock\n",
      "funk/purdieshuffle\n",
      "['funk', 'purdieshuffle']\n",
      "funk\n",
      "latin/venezuelan-joropo\n",
      "['latin', 'venezuelan-joropo']\n",
      "latin\n",
      "afrocuban/rhumba\n",
      "['afrocuban', 'rhumba']\n",
      "afrocuban\n",
      "afrocuban/bembe\n",
      "['afrocuban', 'bembe']\n",
      "afrocuban\n",
      "reggae/slow\n",
      "['reggae', 'slow']\n",
      "reggae\n",
      "latin/reggaeton\n",
      "['latin', 'reggaeton']\n",
      "latin\n",
      "latin/dominican-merengue\n",
      "['latin', 'dominican-merengue']\n",
      "latin\n",
      "latin/bomba\n",
      "['latin', 'bomba']\n",
      "latin\n",
      "latin/merengue\n",
      "['latin', 'merengue']\n",
      "latin\n",
      "funk/rock\n",
      "['funk', 'rock']\n",
      "funk\n",
      "rock\n",
      "neworleans/chacha\n",
      "['neworleans', 'chacha']\n"
     ]
    }
   ],
   "source": [
    "groove_df = groove_df[groove_df.beat_type != 'fill']\n",
    "short = groove_df[groove_df.duration <=30]\n",
    "\n",
    "styles = groove_df['style'].value_counts()\n",
    "\n",
    "#Method 1: Add the multi-style to the first substyle.\n",
    "for s in styles.index:\n",
    "    if '/' in s:\n",
    "        print(s)\n",
    "        style_a,style_b = s.split('/')\n",
    "        print(s.split('/'))\n",
    "        if style_a in styles.index:\n",
    "            split_style = groove_df.query('style==\"'+s+'\"')\n",
    "            groove_df = groove_df.replace({'style':{s:style_a}})\n",
    "        if style_b in styles.index:\n",
    "            split_style = groove_df.query('style==\"'+s+'\"')\n",
    "            groove_df = groove_df.replace({'style':{s:style_b}})\n",
    "\n",
    "styles = groove_df['style'].value_counts()\n",
    "\n",
    "\n",
    "style_durations = pd.DataFrame(columns=['style','max','min','sum'])\n",
    "for s in styles.index:\n",
    "    style_df = groove_df.query('style==\"'+s+'\"')\n",
    "    style_durations = style_durations.append({'style': s, 'max': style_df['duration'].max(), \n",
    "                                             'min': style_df['duration'].min(), 'sum': int(style_df['duration'].sum())},\n",
    "                                             ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_profiling.ProfileReport(groove_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "styles_removed = style_durations[style_durations['sum'] < 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generates MIDI list for each genre\n",
    "genre_midi_list = {}\n",
    "for s in styles.index:\n",
    "    style_df = groove_df.query('style==\"'+s+'\"')\n",
    "    genre_midi_list[s] = style_df.midi_filename.tolist()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---------------------- -----------------------\n",
    "    Machine Learning Prototype And Scaling\n",
    "---------------------- -----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test with one genre\n",
    "r_list = genre_midi_list['rock']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_x = 79\n",
    "max_T_x = 1000\n",
    "sequence_length = 20\n",
    "T_y_generated = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_notes(midi_list):\n",
    "    notes = []\n",
    "    for file in midi_list:\n",
    "        midi = pretty_midi.PrettyMIDI(MIDI_PATH + file)\n",
    "        for instrument in midi.instruments:\n",
    "            if instrument.is_drum:\n",
    "                for note in instrument.notes:\n",
    "                    if int(note.pitch) >= 35 and int(note.pitch) <= 81:\n",
    "                        notes.append((note.pitch))\n",
    "    return notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequences(notes, n_vocab):\n",
    "    sequence_length = 100\n",
    "    print(\"\\n**Preparing sequences for training**\")\n",
    "    pitchnames = sorted(set(i for i in notes)) # list of unique chords and notes\n",
    "    #n_vocab = len(pitchnames)\n",
    "    print(\"Pitchnames (unique notes/chords from 'notes') at length {}: {}\".format(len(pitchnames),pitchnames))\n",
    "    # enumerate pitchnames into dictionary embedding\n",
    "    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n",
    "    print(\"Note to integer embedding created at length {}\".format(len(note_to_int)))\n",
    "\n",
    "    network_input = []\n",
    "    network_output = []\n",
    "\n",
    "    # i equals total notes less declared sequence length of LSTM (ie 5000 - 100)\n",
    "    # sequence input for each i is list of notes i to end of sequence length (ie 0-100 for i = 0)\n",
    "    # sequence output for each i is single note at i + sequence length (ie 100 for i = 0)\n",
    "    for i in range(0, len(notes) - sequence_length,1):\n",
    "        sequence_in = notes[i:i + sequence_length] # 100\n",
    "        sequence_out = notes[i + sequence_length] # 1\n",
    "\n",
    "        # enumerate notes and chord sequences with note_to_int enumerated encoding\n",
    "        # network input/output is a list of encoded notes and chords based on note_to_int encoding\n",
    "        # if 100 unique notes/chords, the encoding will be between 0-100\n",
    "        input_add = [note_to_int[char] for char in sequence_in]\n",
    "        network_input.append(input_add) # sequence length\n",
    "        output_add = note_to_int[sequence_out]\n",
    "        network_output.append(output_add) # single note\n",
    "\n",
    "    print(\"Network input and output created with (pre-transform) lengths {} and {}\".format(len(network_input),len(network_output)))\n",
    "    # print(\"Network input and output first list items: {} and {}\".format(network_input[0],network_output[0]))\n",
    "    # print(\"Network input list item length: {}\".format(len(network_input[0])))\n",
    "    n_patterns = len(network_input) # notes less sequence length\n",
    "    print(\"Lengths. N Vocab: {} N Patterns: {} Pitchnames: {}\".format(n_vocab,n_patterns, len(pitchnames)))\n",
    "    print(\"\\n**Reshaping for training**\")\n",
    "\n",
    "    # convert network input/output from lists to numpy arrays\n",
    "    # reshape input to (notes less sequence length, sequence length)\n",
    "    # reshape output to (notes less sequence length, unique notes/chords)\n",
    "    network_input_r = np.reshape(network_input, (n_patterns, sequence_length, 1))\n",
    "    \n",
    "    #Normalize input\n",
    "    network_input_r = (network_input_r - (float(n_vocab) / 2)) / (float(n_vocab) / 2)\n",
    "    \n",
    "    network_output_r = np_utils.to_categorical(network_output)\n",
    "\n",
    "    print(\"Reshaping network input to (notes - sequence length, sequence length) {}\".format(network_input_r.shape))\n",
    "    print(\"Reshaping network output to (notes - sequence length, unique notes) {}\".format(network_output_r.shape))\n",
    "    return network_input_r, network_output_r, n_patterns, n_vocab, pitchnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{36, 37, 38, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 55, 57, 58, 59}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "notes = get_notes(r_list)\n",
    "n_vocab = len(set(notes))\n",
    "print (set(notes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**Preparing sequences for training**\n",
      "Pitchnames (unique notes/chords from 'notes') at length 20: [36, 37, 38, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 55, 57, 58, 59]\n",
      "Note to integer embedding created at length 20\n",
      "Network input and output created with (pre-transform) lengths 303863 and 303863\n",
      "Lengths. N Vocab: 20 N Patterns: 303863 Pitchnames: 20\n",
      "\n",
      "**Reshaping for training**\n",
      "Reshaping network input to (notes - sequence length, sequence length) (303863, 100, 1)\n",
      "Reshaping network output to (notes - sequence length, unique notes) (303863, 20)\n"
     ]
    }
   ],
   "source": [
    "network_input, network_output, n_patterns, n_vocab, pitchnames = prepare_sequences(notes, n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{55: 2136, 51: 33149, 36: 67336, 44: 37565, 38: 77168, 40: 16797, 37: 8934, 52: 967, 43: 9063, 53: 5053, 42: 26645, 48: 8219, 46: 2166, 59: 1868, 50: 1355, 45: 2843, 47: 1133, 58: 906, 49: 377, 57: 283}\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#importing library\n",
    "from collections import Counter\n",
    "freq = dict(Counter(notes))\n",
    "print (freq)\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#set figure\n",
    "f, ax = plt.subplots(1,1)\n",
    "\n",
    "#plt.figure(figsize=(15,5))\n",
    "plt.bar(freq.keys(), freq.values(), width=.5, color='g')\n",
    "#set title & axis titles\n",
    "ax.set_title('Histogram for pitch', fontsize=20)\n",
    "ax.set_xlabel('Pitch')\n",
    "ax.set_ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN():\n",
    "    def __init__(self, rows):\n",
    "        self.seq_length = rows\n",
    "        self.seq_shape = (self.seq_length, 1)\n",
    "        self.latent_dim = 1000\n",
    "        self.disc_loss = []\n",
    "        self.gen_loss =[]\n",
    "        \n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # The generator takes noise as input and generates note sequence\n",
    "        \n",
    "        z = Input(shape=(self.latent_dim,))\n",
    "        generated_seq = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The discriminator takes generated images as input and determines validity\n",
    "        validity = self.discriminator(generated_seq)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains the generator to fool the discriminator\n",
    "        self.combined = Model(z, validity)\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "        \n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(512, input_shape=self.seq_shape, return_sequences=True))\n",
    "        model.add(Bidirectional(LSTM(512)))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.3))\n",
    "        model.add(Dense(256))\n",
    "        model.add(LeakyReLU(alpha=0.3))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.summary()\n",
    "\n",
    "        seq = Input(shape=self.seq_shape)\n",
    "        validity = model(seq)\n",
    "\n",
    "        return Model(seq, validity)\n",
    "      \n",
    "    def build_generator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Dense(256, input_dim=self.latent_dim))\n",
    "        model.add(LeakyReLU(alpha=0.3))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.3))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(1024))\n",
    "        model.add(LeakyReLU(alpha=0.3))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(np.prod(self.seq_shape), activation='tanh'))\n",
    "        model.add(Reshape(self.seq_shape))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.summary()\n",
    "        \n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        seq = model(noise)\n",
    "\n",
    "        return Model(noise, seq)\n",
    "\n",
    "    def train(self, genre_dataset, epochs, batch_size=128, sample_interval=25):\n",
    "\n",
    "        # Load and convert the data\n",
    "        notes = get_notes(genre_dataset)\n",
    "        n_vocab = len(set(notes))\n",
    "        X_train, y_train, n_patterns, n_vocab, pitchnames = prepare_sequences(notes, n_vocab)\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        real = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "        \n",
    "        # Training the model\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # Training the discriminator\n",
    "            # Select a random batch of note sequences\n",
    "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            real_seqs = X_train[idx]\n",
    "\n",
    "            #noise = np.random.choice(range(484), (batch_size, self.latent_dim))\n",
    "            #noise = (noise-242)/242\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "            \n",
    "            # Generate a batch of new note sequences\n",
    "            \n",
    "            gen_seqs = self.generator.predict(noise)\n",
    "            \n",
    "\n",
    "            # Train the discriminator\n",
    "            d_loss_real = self.discriminator.train_on_batch(real_seqs, real)\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_seqs, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "\n",
    "            #  Training the Generator\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "\n",
    "            #valid_y = np.array([1] * batch_size)\n",
    "            \n",
    "            # Train the generator (to have the discriminator label samples as real)\n",
    "            g_loss = self.combined.train_on_batch(noise, real)\n",
    "\n",
    "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "            # Print the progress and save into loss lists\n",
    "            #if epoch % sample_interval == 0:\n",
    "                #print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "                #self.disc_loss.append(d_loss[0])\n",
    "                #self.gen_loss.append(g_loss)\n",
    "        #self.generate(notes)\n",
    "        #self.plot_loss()\n",
    "        \n",
    "    def plot_loss(self):\n",
    "        plt.plot(self.disc_loss, c='red')\n",
    "        plt.plot(self.gen_loss, c='blue')\n",
    "        plt.title(\"GAN Loss per Epoch\")\n",
    "        plt.legend(['Discriminator', 'Generator'])\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.savefig('GAN_Loss_per_Epoch_final.png', transparent=True)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0411 11:15:10.208863 140715302537024 module_wrapper.py:139] From /home/mark/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0411 11:15:10.211017 140715302537024 module_wrapper.py:139] From /home/mark/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0411 11:15:10.214578 140715302537024 module_wrapper.py:139] From /home/mark/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100, 512)          1052672   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 1024)              4198400   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 5,907,457\n",
      "Trainable params: 5,907,457\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0411 11:15:13.288564 140715302537024 module_wrapper.py:139] From /home/mark/.local/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0411 11:15:13.297116 140715302537024 module_wrapper.py:139] From /home/mark/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0411 11:15:13.304457 140715302537024 deprecation.py:323] From /home/mark/.conda/envs/Springboard/lib/python3.6/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0411 11:15:13.533144 140715302537024 module_wrapper.py:139] From /home/mark/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0411 11:15:13.870439 140715302537024 deprecation.py:506] From /home/mark/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 256)               256256    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 100)               102500    \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 100, 1)            0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100, 1)            0         \n",
      "=================================================================\n",
      "Total params: 1,022,820\n",
      "Trainable params: 1,019,236\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n",
      "\n",
      "**Preparing sequences for training**\n",
      "Pitchnames (unique notes/chords from 'notes') at length 20: [36, 37, 38, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 55, 57, 58, 59]\n",
      "Note to integer embedding created at length 20\n",
      "Network input and output created with (pre-transform) lengths 303863 and 303863\n",
      "Lengths. N Vocab: 20 N Patterns: 303863 Pitchnames: 20\n",
      "\n",
      "**Reshaping for training**\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0411 11:16:35.984421 140715302537024 module_wrapper.py:139] From /home/mark/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "W0411 11:16:35.986739 140715302537024 module_wrapper.py:139] From /home/mark/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0411 11:16:35.989107 140715302537024 module_wrapper.py:139] From /home/mark/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaping network input to (notes - sequence length, sequence length) (303863, 100, 1)\n",
      "Reshaping network output to (notes - sequence length, unique notes) (303863, 20)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0411 11:16:36.333820 140715302537024 module_wrapper.py:139] From /home/mark/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "W0411 11:16:36.336492 140715302537024 module_wrapper.py:139] From /home/mark/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "W0411 11:16:37.153687 140715302537024 module_wrapper.py:139] From /home/mark/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "/home/mark/.local/lib/python3.6/site-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n",
      "W0411 11:16:41.743728 140715302537024 module_wrapper.py:139] From /home/mark/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "W0411 11:16:42.042926 140715302537024 module_wrapper.py:139] From /home/mark/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "/home/mark/.local/lib/python3.6/site-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 0.697647, acc.: 45.31%] [G loss: 0.688935]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mark/.local/lib/python3.6/site-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [D loss: 0.669844, acc.: 75.00%] [G loss: 0.684293]\n",
      "2 [D loss: 0.639244, acc.: 70.31%] [G loss: 0.686888]\n",
      "3 [D loss: 0.602099, acc.: 70.31%] [G loss: 0.702859]\n",
      "4 [D loss: 0.520647, acc.: 78.12%] [G loss: 0.722041]\n",
      "5 [D loss: 0.370057, acc.: 81.25%] [G loss: 1.197731]\n",
      "6 [D loss: 0.576578, acc.: 81.25%] [G loss: 1.118193]\n",
      "7 [D loss: 0.313859, acc.: 95.31%] [G loss: 1.045206]\n",
      "8 [D loss: 0.342583, acc.: 96.88%] [G loss: 1.488775]\n",
      "9 [D loss: 0.226510, acc.: 93.75%] [G loss: 6.804848]\n",
      "10 [D loss: 0.307913, acc.: 89.06%] [G loss: 2.271979]\n",
      "11 [D loss: 0.106092, acc.: 98.44%] [G loss: 3.214393]\n",
      "12 [D loss: 0.093501, acc.: 95.31%] [G loss: 4.017778]\n",
      "13 [D loss: 0.052274, acc.: 98.44%] [G loss: 5.880082]\n",
      "14 [D loss: 0.095330, acc.: 95.31%] [G loss: 9.591565]\n",
      "15 [D loss: 0.159532, acc.: 95.31%] [G loss: 7.888050]\n",
      "16 [D loss: 0.153248, acc.: 92.19%] [G loss: 5.069016]\n",
      "17 [D loss: 0.074165, acc.: 96.88%] [G loss: 8.881224]\n",
      "18 [D loss: 0.123307, acc.: 95.31%] [G loss: 7.692741]\n",
      "19 [D loss: 0.154994, acc.: 93.75%] [G loss: 6.467456]\n",
      "20 [D loss: 0.158643, acc.: 93.75%] [G loss: 4.676102]\n",
      "21 [D loss: 0.137682, acc.: 93.75%] [G loss: 5.968611]\n",
      "22 [D loss: 0.135238, acc.: 92.19%] [G loss: 6.924142]\n",
      "23 [D loss: 0.141213, acc.: 96.88%] [G loss: 4.088996]\n",
      "24 [D loss: 0.147588, acc.: 95.31%] [G loss: 4.977336]\n",
      "25 [D loss: 0.086328, acc.: 98.44%] [G loss: 8.145342]\n",
      "26 [D loss: 0.186758, acc.: 90.62%] [G loss: 5.350234]\n",
      "27 [D loss: 0.165504, acc.: 95.31%] [G loss: 5.475916]\n",
      "28 [D loss: 0.107102, acc.: 95.31%] [G loss: 5.223866]\n",
      "29 [D loss: 0.073215, acc.: 96.88%] [G loss: 8.867472]\n",
      "30 [D loss: 0.076648, acc.: 96.88%] [G loss: 12.416468]\n",
      "31 [D loss: 0.335720, acc.: 92.19%] [G loss: 7.812979]\n",
      "32 [D loss: 0.093937, acc.: 96.88%] [G loss: 6.636609]\n",
      "33 [D loss: 0.355639, acc.: 87.50%] [G loss: 2.018806]\n",
      "34 [D loss: 0.207035, acc.: 92.19%] [G loss: 2.321318]\n",
      "35 [D loss: 0.242816, acc.: 90.62%] [G loss: 3.097036]\n",
      "36 [D loss: 0.119867, acc.: 93.75%] [G loss: 4.096225]\n",
      "37 [D loss: 0.090045, acc.: 96.88%] [G loss: 5.220524]\n",
      "38 [D loss: 0.348329, acc.: 87.50%] [G loss: 2.764893]\n",
      "39 [D loss: 0.164141, acc.: 95.31%] [G loss: 3.155843]\n",
      "40 [D loss: 0.183114, acc.: 90.62%] [G loss: 4.678763]\n",
      "41 [D loss: 0.145374, acc.: 93.75%] [G loss: 5.185251]\n",
      "42 [D loss: 0.199119, acc.: 87.50%] [G loss: 4.604983]\n",
      "43 [D loss: 0.201341, acc.: 89.06%] [G loss: 3.885223]\n",
      "44 [D loss: 0.217023, acc.: 89.06%] [G loss: 3.897874]\n",
      "45 [D loss: 0.133061, acc.: 96.88%] [G loss: 4.339675]\n",
      "46 [D loss: 0.111012, acc.: 95.31%] [G loss: 5.506130]\n",
      "47 [D loss: 0.252826, acc.: 90.62%] [G loss: 3.571602]\n",
      "48 [D loss: 0.187201, acc.: 89.06%] [G loss: 4.110789]\n",
      "49 [D loss: 0.158869, acc.: 95.31%] [G loss: 4.343062]\n",
      "50 [D loss: 0.221347, acc.: 92.19%] [G loss: 3.364771]\n",
      "51 [D loss: 0.163034, acc.: 92.19%] [G loss: 3.443771]\n",
      "52 [D loss: 0.123269, acc.: 93.75%] [G loss: 4.104040]\n",
      "53 [D loss: 0.146079, acc.: 93.75%] [G loss: 3.880066]\n",
      "54 [D loss: 0.097612, acc.: 98.44%] [G loss: 4.483302]\n",
      "55 [D loss: 0.203594, acc.: 89.06%] [G loss: 4.514724]\n",
      "56 [D loss: 0.210296, acc.: 90.62%] [G loss: 3.481216]\n",
      "57 [D loss: 0.096047, acc.: 96.88%] [G loss: 4.888058]\n",
      "58 [D loss: 0.164129, acc.: 93.75%] [G loss: 4.930235]\n",
      "59 [D loss: 0.054171, acc.: 98.44%] [G loss: 6.269619]\n",
      "60 [D loss: 0.078985, acc.: 96.88%] [G loss: 6.798013]\n",
      "61 [D loss: 0.268777, acc.: 90.62%] [G loss: 5.674823]\n",
      "62 [D loss: 0.116760, acc.: 92.19%] [G loss: 4.791458]\n",
      "63 [D loss: 0.188847, acc.: 90.62%] [G loss: 5.142965]\n",
      "64 [D loss: 0.087675, acc.: 98.44%] [G loss: 5.556001]\n",
      "65 [D loss: 0.120224, acc.: 96.88%] [G loss: 7.045460]\n",
      "66 [D loss: 0.146125, acc.: 96.88%] [G loss: 6.393479]\n",
      "67 [D loss: 0.086738, acc.: 96.88%] [G loss: 5.464513]\n",
      "68 [D loss: 0.045711, acc.: 100.00%] [G loss: 7.604466]\n",
      "69 [D loss: 0.350007, acc.: 87.50%] [G loss: 2.722498]\n",
      "70 [D loss: 0.226843, acc.: 90.62%] [G loss: 3.105186]\n",
      "71 [D loss: 0.179925, acc.: 93.75%] [G loss: 4.296285]\n",
      "72 [D loss: 0.080663, acc.: 100.00%] [G loss: 8.376488]\n",
      "73 [D loss: 0.096096, acc.: 95.31%] [G loss: 8.822990]\n",
      "74 [D loss: 0.231414, acc.: 93.75%] [G loss: 3.009097]\n",
      "75 [D loss: 0.193421, acc.: 90.62%] [G loss: 2.822499]\n",
      "76 [D loss: 0.138410, acc.: 93.75%] [G loss: 4.094514]\n",
      "77 [D loss: 0.225347, acc.: 87.50%] [G loss: 4.162948]\n",
      "78 [D loss: 0.167405, acc.: 90.62%] [G loss: 4.417930]\n",
      "79 [D loss: 0.195412, acc.: 92.19%] [G loss: 3.609107]\n",
      "80 [D loss: 0.183597, acc.: 90.62%] [G loss: 4.349537]\n",
      "81 [D loss: 0.293319, acc.: 84.38%] [G loss: 3.670949]\n",
      "82 [D loss: 0.142662, acc.: 95.31%] [G loss: 4.550107]\n",
      "83 [D loss: 0.277118, acc.: 92.19%] [G loss: 3.390435]\n",
      "84 [D loss: 0.181762, acc.: 90.62%] [G loss: 4.187087]\n",
      "85 [D loss: 0.247648, acc.: 92.19%] [G loss: 3.707946]\n",
      "86 [D loss: 0.122179, acc.: 95.31%] [G loss: 4.758257]\n",
      "87 [D loss: 0.111994, acc.: 98.44%] [G loss: 5.670226]\n",
      "88 [D loss: 0.069535, acc.: 98.44%] [G loss: 8.259609]\n",
      "89 [D loss: 0.452597, acc.: 90.62%] [G loss: 2.772744]\n",
      "90 [D loss: 0.127486, acc.: 98.44%] [G loss: 2.730051]\n",
      "91 [D loss: 0.180937, acc.: 92.19%] [G loss: 4.058127]\n",
      "92 [D loss: 0.321207, acc.: 90.62%] [G loss: 2.722804]\n",
      "93 [D loss: 0.201232, acc.: 92.19%] [G loss: 2.737898]\n",
      "94 [D loss: 0.184816, acc.: 93.75%] [G loss: 3.564306]\n",
      "95 [D loss: 0.238880, acc.: 92.19%] [G loss: 3.553833]\n",
      "96 [D loss: 0.138613, acc.: 96.88%] [G loss: 4.585322]\n",
      "97 [D loss: 0.150803, acc.: 93.75%] [G loss: 4.514578]\n",
      "98 [D loss: 0.214019, acc.: 92.19%] [G loss: 3.429412]\n",
      "99 [D loss: 0.267936, acc.: 85.94%] [G loss: 2.992639]\n",
      "100 [D loss: 0.231697, acc.: 87.50%] [G loss: 3.631015]\n",
      "101 [D loss: 0.103675, acc.: 95.31%] [G loss: 4.461563]\n",
      "102 [D loss: 0.257266, acc.: 93.75%] [G loss: 2.673182]\n",
      "103 [D loss: 0.256056, acc.: 90.62%] [G loss: 2.352066]\n",
      "104 [D loss: 0.252029, acc.: 92.19%] [G loss: 3.127625]\n",
      "105 [D loss: 0.173907, acc.: 95.31%] [G loss: 3.970895]\n",
      "106 [D loss: 0.126979, acc.: 96.88%] [G loss: 5.003853]\n",
      "107 [D loss: 0.314074, acc.: 87.50%] [G loss: 3.317810]\n",
      "108 [D loss: 0.176884, acc.: 93.75%] [G loss: 3.424340]\n",
      "109 [D loss: 0.151539, acc.: 93.75%] [G loss: 3.600492]\n",
      "110 [D loss: 0.274323, acc.: 84.38%] [G loss: 3.328807]\n",
      "111 [D loss: 0.173748, acc.: 92.19%] [G loss: 4.129271]\n",
      "112 [D loss: 0.145930, acc.: 92.19%] [G loss: 5.411384]\n",
      "113 [D loss: 0.129476, acc.: 95.31%] [G loss: 6.363201]\n",
      "114 [D loss: 0.175029, acc.: 92.19%] [G loss: 4.572262]\n",
      "115 [D loss: 0.191056, acc.: 90.62%] [G loss: 3.912706]\n",
      "116 [D loss: 0.300780, acc.: 89.06%] [G loss: 2.998182]\n",
      "117 [D loss: 0.127579, acc.: 95.31%] [G loss: 6.280421]\n",
      "118 [D loss: 0.555973, acc.: 89.06%] [G loss: 1.457004]\n",
      "119 [D loss: 0.291607, acc.: 96.88%] [G loss: 1.624186]\n",
      "120 [D loss: 0.391226, acc.: 81.25%] [G loss: 2.419993]\n",
      "121 [D loss: 0.217086, acc.: 89.06%] [G loss: 3.267549]\n",
      "122 [D loss: 0.319067, acc.: 87.50%] [G loss: 2.566421]\n",
      "123 [D loss: 0.315625, acc.: 92.19%] [G loss: 3.176104]\n",
      "124 [D loss: 0.257383, acc.: 87.50%] [G loss: 4.030243]\n",
      "125 [D loss: 0.160818, acc.: 92.19%] [G loss: 4.615514]\n",
      "126 [D loss: 0.256806, acc.: 93.75%] [G loss: 3.213833]\n",
      "127 [D loss: 0.303691, acc.: 87.50%] [G loss: 2.174951]\n",
      "128 [D loss: 0.247258, acc.: 89.06%] [G loss: 2.508106]\n",
      "129 [D loss: 0.328626, acc.: 79.69%] [G loss: 4.901714]\n",
      "130 [D loss: 0.331187, acc.: 90.62%] [G loss: 2.007664]\n",
      "131 [D loss: 0.398236, acc.: 81.25%] [G loss: 2.685779]\n",
      "132 [D loss: 0.185474, acc.: 92.19%] [G loss: 4.480144]\n",
      "133 [D loss: 0.422152, acc.: 81.25%] [G loss: 2.187549]\n",
      "134 [D loss: 0.305128, acc.: 89.06%] [G loss: 3.337967]\n",
      "135 [D loss: 0.578928, acc.: 73.44%] [G loss: 1.276600]\n",
      "136 [D loss: 0.332166, acc.: 85.94%] [G loss: 1.484192]\n",
      "137 [D loss: 0.274941, acc.: 92.19%] [G loss: 2.017787]\n",
      "138 [D loss: 0.375493, acc.: 76.56%] [G loss: 2.841801]\n",
      "139 [D loss: 0.229210, acc.: 92.19%] [G loss: 3.401738]\n",
      "140 [D loss: 0.455224, acc.: 81.25%] [G loss: 2.189949]\n",
      "141 [D loss: 0.286527, acc.: 87.50%] [G loss: 2.259099]\n",
      "142 [D loss: 0.346057, acc.: 85.94%] [G loss: 2.308620]\n",
      "143 [D loss: 0.122618, acc.: 96.88%] [G loss: 5.627816]\n",
      "144 [D loss: 0.220741, acc.: 92.19%] [G loss: 3.135807]\n",
      "145 [D loss: 0.372343, acc.: 85.94%] [G loss: 2.200099]\n",
      "146 [D loss: 0.318776, acc.: 87.50%] [G loss: 2.639671]\n",
      "147 [D loss: 0.340058, acc.: 84.38%] [G loss: 3.406107]\n",
      "148 [D loss: 0.299541, acc.: 85.94%] [G loss: 2.619318]\n",
      "149 [D loss: 0.505423, acc.: 76.56%] [G loss: 1.856694]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150 [D loss: 0.331121, acc.: 84.38%] [G loss: 2.414769]\n",
      "151 [D loss: 0.383035, acc.: 81.25%] [G loss: 2.737596]\n",
      "152 [D loss: 0.427161, acc.: 76.56%] [G loss: 2.017244]\n",
      "153 [D loss: 0.529659, acc.: 81.25%] [G loss: 1.649459]\n",
      "154 [D loss: 0.373550, acc.: 87.50%] [G loss: 2.429234]\n",
      "155 [D loss: 0.216044, acc.: 90.62%] [G loss: 5.915756]\n",
      "156 [D loss: 0.315600, acc.: 85.94%] [G loss: 1.872173]\n",
      "157 [D loss: 0.368196, acc.: 82.81%] [G loss: 1.920852]\n",
      "158 [D loss: 0.355414, acc.: 84.38%] [G loss: 2.247403]\n",
      "159 [D loss: 0.479162, acc.: 78.12%] [G loss: 2.129358]\n",
      "160 [D loss: 0.334282, acc.: 81.25%] [G loss: 2.419474]\n",
      "161 [D loss: 0.317132, acc.: 81.25%] [G loss: 4.044561]\n",
      "162 [D loss: 0.286362, acc.: 82.81%] [G loss: 4.278559]\n",
      "163 [D loss: 0.427259, acc.: 81.25%] [G loss: 1.997857]\n",
      "164 [D loss: 0.258224, acc.: 92.19%] [G loss: 2.497412]\n",
      "165 [D loss: 0.317967, acc.: 81.25%] [G loss: 2.411947]\n",
      "166 [D loss: 0.274996, acc.: 85.94%] [G loss: 3.061620]\n",
      "167 [D loss: 0.458324, acc.: 79.69%] [G loss: 1.938195]\n",
      "168 [D loss: 0.490423, acc.: 81.25%] [G loss: 1.360832]\n",
      "169 [D loss: 0.409533, acc.: 81.25%] [G loss: 1.956361]\n",
      "170 [D loss: 0.423322, acc.: 78.12%] [G loss: 1.686697]\n",
      "171 [D loss: 0.392871, acc.: 82.81%] [G loss: 1.951282]\n",
      "172 [D loss: 0.368592, acc.: 85.94%] [G loss: 2.025140]\n",
      "173 [D loss: 0.379961, acc.: 81.25%] [G loss: 2.369819]\n",
      "174 [D loss: 0.544440, acc.: 79.69%] [G loss: 1.459808]\n",
      "175 [D loss: 0.323427, acc.: 90.62%] [G loss: 2.122933]\n",
      "176 [D loss: 0.352850, acc.: 82.81%] [G loss: 2.490112]\n",
      "177 [D loss: 0.532485, acc.: 73.44%] [G loss: 1.672057]\n",
      "178 [D loss: 0.414128, acc.: 79.69%] [G loss: 4.363653]\n",
      "179 [D loss: 0.447856, acc.: 75.00%] [G loss: 1.635298]\n",
      "180 [D loss: 0.452668, acc.: 71.88%] [G loss: 1.856008]\n",
      "181 [D loss: 0.545627, acc.: 81.25%] [G loss: 1.347862]\n",
      "182 [D loss: 0.465699, acc.: 76.56%] [G loss: 1.935486]\n",
      "183 [D loss: 0.431518, acc.: 75.00%] [G loss: 1.873901]\n",
      "184 [D loss: 0.472833, acc.: 75.00%] [G loss: 1.452236]\n",
      "185 [D loss: 0.561070, acc.: 65.62%] [G loss: 1.135052]\n",
      "186 [D loss: 0.507578, acc.: 73.44%] [G loss: 2.450034]\n",
      "187 [D loss: 0.535861, acc.: 75.00%] [G loss: 1.270092]\n",
      "188 [D loss: 0.383434, acc.: 84.38%] [G loss: 2.149871]\n",
      "189 [D loss: 0.403271, acc.: 81.25%] [G loss: 2.292865]\n",
      "190 [D loss: 0.374425, acc.: 82.81%] [G loss: 1.850480]\n",
      "191 [D loss: 0.513064, acc.: 71.88%] [G loss: 1.875488]\n",
      "192 [D loss: 0.446694, acc.: 73.44%] [G loss: 2.661961]\n",
      "193 [D loss: 0.487988, acc.: 79.69%] [G loss: 2.529895]\n",
      "194 [D loss: 0.416824, acc.: 75.00%] [G loss: 1.711785]\n",
      "195 [D loss: 0.468175, acc.: 71.88%] [G loss: 1.870870]\n",
      "196 [D loss: 0.416074, acc.: 76.56%] [G loss: 2.736587]\n",
      "197 [D loss: 0.477112, acc.: 79.69%] [G loss: 1.580873]\n",
      "198 [D loss: 0.461839, acc.: 78.12%] [G loss: 1.803422]\n",
      "199 [D loss: 0.351139, acc.: 81.25%] [G loss: 3.137429]\n",
      "200 [D loss: 0.651788, acc.: 79.69%] [G loss: 0.997381]\n",
      "201 [D loss: 0.586273, acc.: 71.88%] [G loss: 0.943046]\n",
      "202 [D loss: 0.550182, acc.: 71.88%] [G loss: 0.983791]\n",
      "203 [D loss: 0.438502, acc.: 81.25%] [G loss: 1.103606]\n",
      "204 [D loss: 0.533857, acc.: 75.00%] [G loss: 1.275298]\n",
      "205 [D loss: 0.546275, acc.: 64.06%] [G loss: 1.224949]\n",
      "206 [D loss: 0.484868, acc.: 79.69%] [G loss: 1.273203]\n",
      "207 [D loss: 0.632141, acc.: 68.75%] [G loss: 1.350427]\n",
      "208 [D loss: 0.422847, acc.: 79.69%] [G loss: 1.549339]\n",
      "209 [D loss: 0.578114, acc.: 68.75%] [G loss: 1.580541]\n",
      "210 [D loss: 0.488959, acc.: 73.44%] [G loss: 1.965649]\n",
      "211 [D loss: 0.631783, acc.: 64.06%] [G loss: 1.402774]\n",
      "212 [D loss: 0.608800, acc.: 67.19%] [G loss: 1.315521]\n",
      "213 [D loss: 0.625893, acc.: 60.94%] [G loss: 1.341777]\n",
      "214 [D loss: 0.552609, acc.: 75.00%] [G loss: 1.601978]\n",
      "215 [D loss: 0.465510, acc.: 76.56%] [G loss: 1.687124]\n",
      "216 [D loss: 0.526735, acc.: 79.69%] [G loss: 1.577932]\n",
      "217 [D loss: 0.532827, acc.: 76.56%] [G loss: 1.445246]\n",
      "218 [D loss: 0.615373, acc.: 67.19%] [G loss: 1.423768]\n",
      "219 [D loss: 0.566518, acc.: 64.06%] [G loss: 1.572470]\n",
      "220 [D loss: 0.668944, acc.: 54.69%] [G loss: 1.394870]\n",
      "221 [D loss: 0.437467, acc.: 75.00%] [G loss: 2.076247]\n",
      "222 [D loss: 0.523515, acc.: 70.31%] [G loss: 1.931801]\n",
      "223 [D loss: 0.587878, acc.: 67.19%] [G loss: 1.667661]\n",
      "224 [D loss: 0.489902, acc.: 78.12%] [G loss: 2.092881]\n",
      "225 [D loss: 0.593095, acc.: 59.38%] [G loss: 1.757041]\n",
      "226 [D loss: 0.629356, acc.: 64.06%] [G loss: 1.445364]\n",
      "227 [D loss: 0.486008, acc.: 70.31%] [G loss: 2.685513]\n",
      "228 [D loss: 0.600976, acc.: 70.31%] [G loss: 1.133472]\n",
      "229 [D loss: 0.619630, acc.: 60.94%] [G loss: 1.104522]\n",
      "230 [D loss: 0.527477, acc.: 71.88%] [G loss: 1.293781]\n",
      "231 [D loss: 0.612734, acc.: 60.94%] [G loss: 1.824754]\n",
      "232 [D loss: 0.450321, acc.: 78.12%] [G loss: 3.911559]\n",
      "233 [D loss: 0.555481, acc.: 73.44%] [G loss: 1.517855]\n",
      "234 [D loss: 0.539196, acc.: 65.62%] [G loss: 1.619281]\n",
      "235 [D loss: 0.469775, acc.: 75.00%] [G loss: 2.419514]\n",
      "236 [D loss: 0.459235, acc.: 76.56%] [G loss: 2.648552]\n",
      "237 [D loss: 0.511558, acc.: 73.44%] [G loss: 2.864524]\n",
      "238 [D loss: 0.684858, acc.: 54.69%] [G loss: 1.017902]\n",
      "239 [D loss: 0.632028, acc.: 62.50%] [G loss: 1.017784]\n",
      "240 [D loss: 0.603457, acc.: 62.50%] [G loss: 1.103481]\n",
      "241 [D loss: 0.494464, acc.: 76.56%] [G loss: 1.925680]\n",
      "242 [D loss: 0.619887, acc.: 68.75%] [G loss: 1.117495]\n",
      "243 [D loss: 0.543579, acc.: 67.19%] [G loss: 1.165842]\n",
      "244 [D loss: 0.568733, acc.: 71.88%] [G loss: 1.240806]\n",
      "245 [D loss: 0.558385, acc.: 73.44%] [G loss: 1.456402]\n",
      "246 [D loss: 0.653535, acc.: 56.25%] [G loss: 1.625998]\n",
      "247 [D loss: 0.674213, acc.: 59.38%] [G loss: 1.347118]\n",
      "248 [D loss: 0.572965, acc.: 71.88%] [G loss: 1.418624]\n",
      "249 [D loss: 0.555605, acc.: 70.31%] [G loss: 2.184479]\n",
      "250 [D loss: 0.664896, acc.: 67.19%] [G loss: 1.133643]\n",
      "251 [D loss: 0.504423, acc.: 70.31%] [G loss: 1.160175]\n",
      "252 [D loss: 0.537416, acc.: 67.19%] [G loss: 1.357635]\n",
      "253 [D loss: 0.598042, acc.: 64.06%] [G loss: 1.378253]\n",
      "254 [D loss: 0.598611, acc.: 67.19%] [G loss: 1.430230]\n",
      "255 [D loss: 0.515686, acc.: 70.31%] [G loss: 1.710499]\n",
      "256 [D loss: 0.506050, acc.: 71.88%] [G loss: 2.856755]\n",
      "257 [D loss: 0.582084, acc.: 68.75%] [G loss: 1.441331]\n",
      "258 [D loss: 0.494150, acc.: 73.44%] [G loss: 1.424675]\n",
      "259 [D loss: 0.556798, acc.: 70.31%] [G loss: 1.571604]\n",
      "260 [D loss: 0.557342, acc.: 68.75%] [G loss: 1.425620]\n",
      "261 [D loss: 0.476378, acc.: 78.12%] [G loss: 1.501898]\n",
      "262 [D loss: 0.626429, acc.: 67.19%] [G loss: 1.134933]\n",
      "263 [D loss: 1.231537, acc.: 50.00%] [G loss: 16.054111]\n",
      "264 [D loss: 7.985820, acc.: 50.00%] [G loss: 15.702978]\n",
      "265 [D loss: 8.037111, acc.: 50.00%] [G loss: 12.407867]\n",
      "266 [D loss: 5.263948, acc.: 18.75%] [G loss: 0.865075]\n",
      "267 [D loss: 0.733099, acc.: 35.94%] [G loss: 1.119439]\n",
      "268 [D loss: 0.707641, acc.: 56.25%] [G loss: 0.856204]\n",
      "269 [D loss: 0.605175, acc.: 65.62%] [G loss: 0.913116]\n",
      "270 [D loss: 0.635597, acc.: 64.06%] [G loss: 1.062060]\n",
      "271 [D loss: 0.561867, acc.: 67.19%] [G loss: 1.177587]\n",
      "272 [D loss: 0.626783, acc.: 57.81%] [G loss: 1.219611]\n",
      "273 [D loss: 0.619949, acc.: 67.19%] [G loss: 1.381557]\n",
      "274 [D loss: 0.579305, acc.: 65.62%] [G loss: 1.595203]\n",
      "275 [D loss: 0.610740, acc.: 68.75%] [G loss: 1.210125]\n",
      "276 [D loss: 0.652730, acc.: 56.25%] [G loss: 1.260978]\n",
      "277 [D loss: 0.587198, acc.: 65.62%] [G loss: 1.440000]\n",
      "278 [D loss: 0.606217, acc.: 68.75%] [G loss: 1.731102]\n",
      "279 [D loss: 0.736721, acc.: 59.38%] [G loss: 1.090415]\n",
      "280 [D loss: 0.608645, acc.: 70.31%] [G loss: 1.134011]\n",
      "281 [D loss: 0.622966, acc.: 59.38%] [G loss: 1.105007]\n",
      "282 [D loss: 0.599424, acc.: 65.62%] [G loss: 1.306982]\n",
      "283 [D loss: 0.609408, acc.: 64.06%] [G loss: 1.477227]\n",
      "284 [D loss: 0.644261, acc.: 57.81%] [G loss: 0.991128]\n",
      "285 [D loss: 0.633521, acc.: 64.06%] [G loss: 1.033014]\n",
      "286 [D loss: 0.649287, acc.: 54.69%] [G loss: 1.186924]\n",
      "287 [D loss: 0.579977, acc.: 68.75%] [G loss: 1.451029]\n",
      "288 [D loss: 0.583027, acc.: 67.19%] [G loss: 1.835571]\n",
      "289 [D loss: 0.527014, acc.: 70.31%] [G loss: 1.476387]\n",
      "290 [D loss: 0.669377, acc.: 60.94%] [G loss: 1.342760]\n",
      "291 [D loss: 0.635038, acc.: 65.62%] [G loss: 1.552656]\n",
      "292 [D loss: 0.603675, acc.: 65.62%] [G loss: 1.557163]\n",
      "293 [D loss: 0.516525, acc.: 73.44%] [G loss: 1.759359]\n",
      "294 [D loss: 0.588353, acc.: 70.31%] [G loss: 1.655543]\n",
      "295 [D loss: 0.618245, acc.: 65.62%] [G loss: 2.149376]\n",
      "296 [D loss: 0.573383, acc.: 71.88%] [G loss: 1.463291]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "297 [D loss: 0.587810, acc.: 68.75%] [G loss: 1.066630]\n",
      "298 [D loss: 0.542312, acc.: 73.44%] [G loss: 2.030416]\n",
      "299 [D loss: 0.493800, acc.: 76.56%] [G loss: 3.185400]\n",
      "300 [D loss: 0.578702, acc.: 70.31%] [G loss: 4.942839]\n",
      "301 [D loss: 1.267360, acc.: 53.12%] [G loss: 0.886079]\n",
      "302 [D loss: 0.613269, acc.: 75.00%] [G loss: 0.880686]\n",
      "303 [D loss: 0.592830, acc.: 75.00%] [G loss: 0.913561]\n",
      "304 [D loss: 0.616933, acc.: 68.75%] [G loss: 1.047657]\n",
      "305 [D loss: 0.587418, acc.: 67.19%] [G loss: 1.097372]\n",
      "306 [D loss: 0.624438, acc.: 67.19%] [G loss: 1.254512]\n",
      "307 [D loss: 0.566653, acc.: 71.88%] [G loss: 1.256783]\n",
      "308 [D loss: 0.599887, acc.: 70.31%] [G loss: 1.402540]\n",
      "309 [D loss: 0.595987, acc.: 67.19%] [G loss: 1.464673]\n",
      "310 [D loss: 0.648620, acc.: 60.94%] [G loss: 1.372750]\n",
      "311 [D loss: 0.574776, acc.: 67.19%] [G loss: 1.335840]\n",
      "312 [D loss: 0.613598, acc.: 65.62%] [G loss: 1.517033]\n",
      "313 [D loss: 0.549271, acc.: 73.44%] [G loss: 1.472196]\n",
      "314 [D loss: 0.586981, acc.: 67.19%] [G loss: 1.735252]\n",
      "315 [D loss: 0.607754, acc.: 65.62%] [G loss: 1.420929]\n",
      "316 [D loss: 0.571335, acc.: 73.44%] [G loss: 1.687453]\n",
      "317 [D loss: 0.637872, acc.: 65.62%] [G loss: 1.539644]\n",
      "318 [D loss: 0.530125, acc.: 71.88%] [G loss: 1.968405]\n",
      "319 [D loss: 0.611611, acc.: 64.06%] [G loss: 1.552328]\n",
      "320 [D loss: 0.611490, acc.: 64.06%] [G loss: 1.413027]\n",
      "321 [D loss: 0.626284, acc.: 67.19%] [G loss: 1.534326]\n",
      "322 [D loss: 0.567023, acc.: 75.00%] [G loss: 1.505195]\n",
      "323 [D loss: 0.591630, acc.: 67.19%] [G loss: 1.213963]\n",
      "324 [D loss: 0.667520, acc.: 57.81%] [G loss: 1.609434]\n",
      "325 [D loss: 0.582970, acc.: 73.44%] [G loss: 1.143568]\n",
      "326 [D loss: 0.616200, acc.: 65.62%] [G loss: 1.291983]\n",
      "327 [D loss: 0.646573, acc.: 62.50%] [G loss: 1.239132]\n",
      "328 [D loss: 0.585212, acc.: 70.31%] [G loss: 1.248424]\n",
      "329 [D loss: 0.642289, acc.: 60.94%] [G loss: 1.632367]\n",
      "330 [D loss: 0.645417, acc.: 59.38%] [G loss: 1.463566]\n",
      "331 [D loss: 0.613452, acc.: 67.19%] [G loss: 1.592677]\n",
      "332 [D loss: 0.605730, acc.: 65.62%] [G loss: 1.530465]\n",
      "333 [D loss: 0.595166, acc.: 67.19%] [G loss: 1.740385]\n",
      "334 [D loss: 0.622257, acc.: 60.94%] [G loss: 1.673873]\n",
      "335 [D loss: 0.653280, acc.: 60.94%] [G loss: 1.685573]\n",
      "336 [D loss: 0.515811, acc.: 78.12%] [G loss: 1.955326]\n",
      "337 [D loss: 0.594740, acc.: 73.44%] [G loss: 1.375408]\n",
      "338 [D loss: 0.620795, acc.: 65.62%] [G loss: 1.460455]\n",
      "339 [D loss: 0.642358, acc.: 60.94%] [G loss: 1.081462]\n",
      "340 [D loss: 0.615565, acc.: 64.06%] [G loss: 0.995290]\n",
      "341 [D loss: 0.621387, acc.: 65.62%] [G loss: 1.306674]\n",
      "342 [D loss: 0.601767, acc.: 67.19%] [G loss: 1.447539]\n",
      "343 [D loss: 0.609531, acc.: 67.19%] [G loss: 1.304932]\n",
      "344 [D loss: 0.614707, acc.: 64.06%] [G loss: 1.414941]\n",
      "345 [D loss: 0.609036, acc.: 65.62%] [G loss: 1.562849]\n",
      "346 [D loss: 0.529701, acc.: 76.56%] [G loss: 1.562513]\n",
      "347 [D loss: 0.580945, acc.: 70.31%] [G loss: 1.508356]\n",
      "348 [D loss: 0.648076, acc.: 67.19%] [G loss: 1.683369]\n",
      "349 [D loss: 0.627161, acc.: 64.06%] [G loss: 1.304810]\n",
      "350 [D loss: 0.583509, acc.: 62.50%] [G loss: 1.518141]\n",
      "351 [D loss: 0.719141, acc.: 59.38%] [G loss: 1.097729]\n",
      "352 [D loss: 0.603871, acc.: 67.19%] [G loss: 1.007516]\n",
      "353 [D loss: 0.647502, acc.: 57.81%] [G loss: 1.129641]\n",
      "354 [D loss: 0.579245, acc.: 73.44%] [G loss: 1.231963]\n",
      "355 [D loss: 0.601785, acc.: 64.06%] [G loss: 1.262895]\n",
      "356 [D loss: 0.629162, acc.: 59.38%] [G loss: 1.339425]\n",
      "357 [D loss: 0.543717, acc.: 71.88%] [G loss: 1.543619]\n",
      "358 [D loss: 0.617956, acc.: 67.19%] [G loss: 1.892414]\n",
      "359 [D loss: 0.634794, acc.: 67.19%] [G loss: 1.697545]\n",
      "360 [D loss: 0.587886, acc.: 70.31%] [G loss: 1.599452]\n",
      "361 [D loss: 0.638100, acc.: 70.31%] [G loss: 1.381663]\n",
      "362 [D loss: 0.517519, acc.: 73.44%] [G loss: 1.554332]\n",
      "363 [D loss: 0.542335, acc.: 73.44%] [G loss: 1.348648]\n",
      "364 [D loss: 0.517891, acc.: 64.06%] [G loss: 1.459564]\n",
      "365 [D loss: 0.573376, acc.: 67.19%] [G loss: 1.678972]\n",
      "366 [D loss: 0.595460, acc.: 70.31%] [G loss: 1.709684]\n",
      "367 [D loss: 0.639022, acc.: 64.06%] [G loss: 1.497649]\n",
      "368 [D loss: 0.588555, acc.: 70.31%] [G loss: 1.434079]\n",
      "369 [D loss: 0.607733, acc.: 60.94%] [G loss: 1.165276]\n",
      "370 [D loss: 0.633421, acc.: 59.38%] [G loss: 1.589101]\n",
      "371 [D loss: 0.570822, acc.: 68.75%] [G loss: 1.479287]\n",
      "372 [D loss: 0.701778, acc.: 56.25%] [G loss: 0.841438]\n",
      "373 [D loss: 0.640827, acc.: 59.38%] [G loss: 0.896600]\n",
      "374 [D loss: 0.595828, acc.: 70.31%] [G loss: 0.893389]\n",
      "375 [D loss: 0.627396, acc.: 57.81%] [G loss: 1.072213]\n",
      "376 [D loss: 0.644667, acc.: 60.94%] [G loss: 0.921190]\n",
      "377 [D loss: 0.614800, acc.: 67.19%] [G loss: 0.770049]\n",
      "378 [D loss: 0.605395, acc.: 65.62%] [G loss: 0.864221]\n",
      "379 [D loss: 0.628960, acc.: 60.94%] [G loss: 1.002959]\n",
      "380 [D loss: 0.594118, acc.: 70.31%] [G loss: 1.130208]\n",
      "381 [D loss: 0.576080, acc.: 65.62%] [G loss: 1.112610]\n",
      "382 [D loss: 0.569172, acc.: 65.62%] [G loss: 1.194194]\n",
      "383 [D loss: 0.829578, acc.: 53.12%] [G loss: 0.971802]\n",
      "384 [D loss: 0.614552, acc.: 62.50%] [G loss: 0.946338]\n",
      "385 [D loss: 0.590013, acc.: 68.75%] [G loss: 1.227547]\n",
      "386 [D loss: 0.608542, acc.: 62.50%] [G loss: 1.410175]\n",
      "387 [D loss: 0.577288, acc.: 68.75%] [G loss: 1.608389]\n",
      "388 [D loss: 0.558721, acc.: 71.88%] [G loss: 1.295617]\n",
      "389 [D loss: 0.576488, acc.: 75.00%] [G loss: 0.832036]\n",
      "390 [D loss: 0.583253, acc.: 71.88%] [G loss: 0.781074]\n",
      "391 [D loss: 0.667959, acc.: 60.94%] [G loss: 0.867342]\n",
      "392 [D loss: 0.613530, acc.: 68.75%] [G loss: 0.806171]\n",
      "393 [D loss: 0.598509, acc.: 67.19%] [G loss: 0.867315]\n",
      "394 [D loss: 0.615310, acc.: 62.50%] [G loss: 0.804971]\n",
      "395 [D loss: 0.667349, acc.: 51.56%] [G loss: 1.055335]\n",
      "396 [D loss: 0.599922, acc.: 62.50%] [G loss: 1.028565]\n",
      "397 [D loss: 0.609054, acc.: 70.31%] [G loss: 0.932732]\n",
      "398 [D loss: 0.618541, acc.: 65.62%] [G loss: 0.927389]\n",
      "399 [D loss: 0.623324, acc.: 70.31%] [G loss: 0.974246]\n",
      "400 [D loss: 0.567873, acc.: 76.56%] [G loss: 0.911233]\n",
      "401 [D loss: 0.650064, acc.: 56.25%] [G loss: 0.986951]\n",
      "402 [D loss: 0.609672, acc.: 68.75%] [G loss: 0.857401]\n",
      "403 [D loss: 0.534001, acc.: 70.31%] [G loss: 1.011554]\n",
      "404 [D loss: 0.542869, acc.: 75.00%] [G loss: 1.077055]\n",
      "405 [D loss: 0.585472, acc.: 65.62%] [G loss: 1.129168]\n",
      "406 [D loss: 0.562736, acc.: 71.88%] [G loss: 1.150288]\n",
      "407 [D loss: 0.601864, acc.: 64.06%] [G loss: 1.421032]\n",
      "408 [D loss: 0.646523, acc.: 60.94%] [G loss: 1.216847]\n",
      "409 [D loss: 0.609727, acc.: 62.50%] [G loss: 1.332803]\n",
      "410 [D loss: 0.627722, acc.: 60.94%] [G loss: 1.333197]\n",
      "411 [D loss: 0.675444, acc.: 60.94%] [G loss: 1.011015]\n",
      "412 [D loss: 0.632098, acc.: 65.62%] [G loss: 1.177992]\n",
      "413 [D loss: 0.660264, acc.: 60.94%] [G loss: 1.132742]\n",
      "414 [D loss: 0.595327, acc.: 64.06%] [G loss: 1.094415]\n",
      "415 [D loss: 0.638960, acc.: 59.38%] [G loss: 1.492158]\n",
      "416 [D loss: 0.678646, acc.: 56.25%] [G loss: 1.563550]\n",
      "417 [D loss: 0.713108, acc.: 62.50%] [G loss: 1.667878]\n",
      "418 [D loss: 0.553407, acc.: 75.00%] [G loss: 1.556194]\n",
      "419 [D loss: 0.543777, acc.: 73.44%] [G loss: 1.609802]\n",
      "420 [D loss: 0.600180, acc.: 65.62%] [G loss: 1.781340]\n",
      "421 [D loss: 0.620972, acc.: 60.94%] [G loss: 1.792751]\n",
      "422 [D loss: 0.535192, acc.: 70.31%] [G loss: 1.753780]\n",
      "423 [D loss: 0.608100, acc.: 59.38%] [G loss: 2.519783]\n",
      "424 [D loss: 0.604787, acc.: 65.62%] [G loss: 2.077358]\n",
      "425 [D loss: 0.655303, acc.: 64.06%] [G loss: 1.781155]\n",
      "426 [D loss: 0.670836, acc.: 65.62%] [G loss: 1.458399]\n",
      "427 [D loss: 0.632677, acc.: 59.38%] [G loss: 1.060108]\n",
      "428 [D loss: 0.626394, acc.: 60.94%] [G loss: 1.310313]\n",
      "429 [D loss: 0.610768, acc.: 57.81%] [G loss: 1.118963]\n",
      "430 [D loss: 0.621956, acc.: 65.62%] [G loss: 1.450742]\n",
      "431 [D loss: 0.697198, acc.: 54.69%] [G loss: 1.514735]\n",
      "432 [D loss: 0.557875, acc.: 67.19%] [G loss: 1.757379]\n",
      "433 [D loss: 0.587073, acc.: 60.94%] [G loss: 1.855443]\n",
      "434 [D loss: 0.608438, acc.: 65.62%] [G loss: 1.795320]\n",
      "435 [D loss: 0.673172, acc.: 59.38%] [G loss: 1.682137]\n",
      "436 [D loss: 0.611031, acc.: 64.06%] [G loss: 1.593385]\n",
      "437 [D loss: 0.589304, acc.: 62.50%] [G loss: 1.586336]\n",
      "438 [D loss: 0.614167, acc.: 57.81%] [G loss: 1.853538]\n",
      "439 [D loss: 0.626149, acc.: 68.75%] [G loss: 0.898765]\n",
      "440 [D loss: 0.630041, acc.: 65.62%] [G loss: 0.961428]\n",
      "441 [D loss: 0.614314, acc.: 65.62%] [G loss: 1.093478]\n",
      "442 [D loss: 0.652249, acc.: 59.38%] [G loss: 0.900684]\n",
      "443 [D loss: 0.560992, acc.: 73.44%] [G loss: 1.053087]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "444 [D loss: 0.625899, acc.: 56.25%] [G loss: 0.896886]\n",
      "445 [D loss: 0.650359, acc.: 62.50%] [G loss: 1.196354]\n",
      "446 [D loss: 0.622614, acc.: 64.06%] [G loss: 1.364190]\n",
      "447 [D loss: 0.679702, acc.: 53.12%] [G loss: 1.092845]\n",
      "448 [D loss: 0.622155, acc.: 62.50%] [G loss: 1.168899]\n",
      "449 [D loss: 0.647242, acc.: 60.94%] [G loss: 1.184201]\n",
      "450 [D loss: 0.658575, acc.: 60.94%] [G loss: 1.116176]\n",
      "451 [D loss: 0.612692, acc.: 60.94%] [G loss: 1.175390]\n",
      "452 [D loss: 0.621543, acc.: 65.62%] [G loss: 1.565444]\n",
      "453 [D loss: 0.627883, acc.: 64.06%] [G loss: 1.719118]\n",
      "454 [D loss: 0.693587, acc.: 65.62%] [G loss: 1.781160]\n",
      "455 [D loss: 0.654352, acc.: 67.19%] [G loss: 1.176485]\n",
      "456 [D loss: 0.623841, acc.: 59.38%] [G loss: 1.463847]\n",
      "457 [D loss: 0.622097, acc.: 70.31%] [G loss: 0.856058]\n",
      "458 [D loss: 0.629973, acc.: 68.75%] [G loss: 0.832319]\n",
      "459 [D loss: 0.612794, acc.: 67.19%] [G loss: 0.849267]\n",
      "460 [D loss: 0.637054, acc.: 62.50%] [G loss: 0.919051]\n",
      "461 [D loss: 0.604930, acc.: 71.88%] [G loss: 0.924983]\n",
      "462 [D loss: 0.596171, acc.: 71.88%] [G loss: 1.125032]\n",
      "463 [D loss: 0.709477, acc.: 53.12%] [G loss: 0.978672]\n",
      "464 [D loss: 0.636167, acc.: 59.38%] [G loss: 0.915917]\n",
      "465 [D loss: 0.669303, acc.: 62.50%] [G loss: 0.968173]\n",
      "466 [D loss: 0.582842, acc.: 67.19%] [G loss: 1.145195]\n",
      "467 [D loss: 0.634761, acc.: 71.88%] [G loss: 1.110541]\n",
      "468 [D loss: 0.605476, acc.: 67.19%] [G loss: 1.280268]\n",
      "469 [D loss: 0.551767, acc.: 75.00%] [G loss: 1.519053]\n",
      "470 [D loss: 0.768942, acc.: 42.19%] [G loss: 0.854020]\n",
      "471 [D loss: 0.663257, acc.: 59.38%] [G loss: 0.767594]\n",
      "472 [D loss: 0.666354, acc.: 59.38%] [G loss: 0.792302]\n",
      "473 [D loss: 0.687309, acc.: 53.12%] [G loss: 0.877094]\n",
      "474 [D loss: 0.653215, acc.: 62.50%] [G loss: 0.835546]\n",
      "475 [D loss: 0.655380, acc.: 64.06%] [G loss: 0.873802]\n",
      "476 [D loss: 0.665168, acc.: 59.38%] [G loss: 0.883264]\n",
      "477 [D loss: 0.667784, acc.: 59.38%] [G loss: 0.897918]\n",
      "478 [D loss: 0.619210, acc.: 71.88%] [G loss: 0.862068]\n",
      "479 [D loss: 0.679077, acc.: 57.81%] [G loss: 0.931403]\n",
      "480 [D loss: 0.609548, acc.: 71.88%] [G loss: 0.985062]\n",
      "481 [D loss: 0.653266, acc.: 59.38%] [G loss: 0.929084]\n",
      "482 [D loss: 0.636242, acc.: 65.62%] [G loss: 0.874284]\n",
      "483 [D loss: 0.630813, acc.: 60.94%] [G loss: 0.922178]\n",
      "484 [D loss: 0.671605, acc.: 54.69%] [G loss: 1.067705]\n",
      "485 [D loss: 0.632629, acc.: 64.06%] [G loss: 1.038318]\n",
      "486 [D loss: 0.662051, acc.: 56.25%] [G loss: 1.299700]\n",
      "487 [D loss: 0.695634, acc.: 65.62%] [G loss: 1.268802]\n",
      "488 [D loss: 0.644846, acc.: 57.81%] [G loss: 1.144366]\n",
      "489 [D loss: 0.614873, acc.: 60.94%] [G loss: 1.299356]\n",
      "490 [D loss: 0.602873, acc.: 62.50%] [G loss: 1.356350]\n",
      "491 [D loss: 0.627048, acc.: 60.94%] [G loss: 1.681483]\n",
      "492 [D loss: 0.529967, acc.: 78.12%] [G loss: 2.013702]\n",
      "493 [D loss: 0.845409, acc.: 50.00%] [G loss: 1.040443]\n",
      "494 [D loss: 0.632157, acc.: 59.38%] [G loss: 1.164809]\n",
      "495 [D loss: 0.648496, acc.: 64.06%] [G loss: 1.028276]\n",
      "496 [D loss: 0.643035, acc.: 67.19%] [G loss: 0.993904]\n",
      "497 [D loss: 0.682650, acc.: 57.81%] [G loss: 1.044152]\n",
      "498 [D loss: 0.680756, acc.: 60.94%] [G loss: 1.012955]\n",
      "499 [D loss: 0.631107, acc.: 62.50%] [G loss: 1.084050]\n",
      "500 [D loss: 0.681941, acc.: 57.81%] [G loss: 1.022008]\n",
      "501 [D loss: 0.648373, acc.: 64.06%] [G loss: 1.090441]\n",
      "502 [D loss: 0.649396, acc.: 56.25%] [G loss: 1.125186]\n",
      "503 [D loss: 0.695739, acc.: 59.38%] [G loss: 1.138612]\n",
      "504 [D loss: 0.684630, acc.: 56.25%] [G loss: 1.191561]\n",
      "505 [D loss: 0.641324, acc.: 65.62%] [G loss: 1.165640]\n",
      "506 [D loss: 0.700117, acc.: 56.25%] [G loss: 1.167484]\n",
      "507 [D loss: 0.649213, acc.: 60.94%] [G loss: 1.250028]\n",
      "508 [D loss: 0.672555, acc.: 53.12%] [G loss: 1.373099]\n",
      "509 [D loss: 0.658334, acc.: 57.81%] [G loss: 1.417300]\n",
      "510 [D loss: 0.679954, acc.: 54.69%] [G loss: 1.334598]\n",
      "511 [D loss: 0.641828, acc.: 62.50%] [G loss: 1.362245]\n",
      "512 [D loss: 0.689847, acc.: 60.94%] [G loss: 1.378515]\n",
      "513 [D loss: 0.674644, acc.: 62.50%] [G loss: 1.338187]\n",
      "514 [D loss: 0.590945, acc.: 68.75%] [G loss: 1.459558]\n",
      "515 [D loss: 0.613655, acc.: 71.88%] [G loss: 1.581203]\n",
      "516 [D loss: 0.796603, acc.: 56.25%] [G loss: 1.654632]\n",
      "517 [D loss: 0.681671, acc.: 59.38%] [G loss: 1.334628]\n",
      "518 [D loss: 0.739729, acc.: 54.69%] [G loss: 1.156629]\n",
      "519 [D loss: 0.683382, acc.: 65.62%] [G loss: 1.112146]\n",
      "520 [D loss: 0.652087, acc.: 57.81%] [G loss: 1.077439]\n",
      "521 [D loss: 0.660202, acc.: 65.62%] [G loss: 1.128390]\n",
      "522 [D loss: 0.635456, acc.: 64.06%] [G loss: 1.358647]\n",
      "523 [D loss: 0.649554, acc.: 60.94%] [G loss: 1.839043]\n",
      "524 [D loss: 0.615001, acc.: 70.31%] [G loss: 2.067588]\n",
      "525 [D loss: 0.712090, acc.: 53.12%] [G loss: 1.166559]\n",
      "526 [D loss: 0.688519, acc.: 57.81%] [G loss: 1.288902]\n",
      "527 [D loss: 0.610510, acc.: 62.50%] [G loss: 1.383759]\n",
      "528 [D loss: 0.652772, acc.: 60.94%] [G loss: 1.829571]\n",
      "529 [D loss: 0.651771, acc.: 64.06%] [G loss: 1.576237]\n",
      "530 [D loss: 0.591029, acc.: 68.75%] [G loss: 1.853644]\n",
      "531 [D loss: 0.696340, acc.: 62.50%] [G loss: 1.431274]\n",
      "532 [D loss: 0.578425, acc.: 73.44%] [G loss: 1.539813]\n",
      "533 [D loss: 0.655600, acc.: 62.50%] [G loss: 1.469290]\n",
      "534 [D loss: 0.606944, acc.: 68.75%] [G loss: 1.578912]\n",
      "535 [D loss: 0.667442, acc.: 60.94%] [G loss: 1.600876]\n",
      "536 [D loss: 0.599924, acc.: 64.06%] [G loss: 1.837382]\n",
      "537 [D loss: 0.566715, acc.: 70.31%] [G loss: 2.404360]\n",
      "538 [D loss: 0.571395, acc.: 73.44%] [G loss: 3.306993]\n",
      "539 [D loss: 1.003871, acc.: 46.88%] [G loss: 0.891044]\n",
      "540 [D loss: 0.644766, acc.: 67.19%] [G loss: 0.856347]\n",
      "541 [D loss: 0.656741, acc.: 60.94%] [G loss: 0.815726]\n",
      "542 [D loss: 0.649918, acc.: 62.50%] [G loss: 0.908234]\n",
      "543 [D loss: 0.692492, acc.: 56.25%] [G loss: 0.839847]\n",
      "544 [D loss: 0.655254, acc.: 59.38%] [G loss: 0.922116]\n",
      "545 [D loss: 0.667506, acc.: 64.06%] [G loss: 0.911198]\n",
      "546 [D loss: 0.708258, acc.: 56.25%] [G loss: 0.926465]\n",
      "547 [D loss: 0.663185, acc.: 62.50%] [G loss: 0.951486]\n",
      "548 [D loss: 0.614607, acc.: 75.00%] [G loss: 0.999870]\n",
      "549 [D loss: 0.605699, acc.: 71.88%] [G loss: 0.951490]\n",
      "550 [D loss: 0.674145, acc.: 62.50%] [G loss: 1.020457]\n",
      "551 [D loss: 0.641851, acc.: 62.50%] [G loss: 0.990315]\n",
      "552 [D loss: 0.636116, acc.: 65.62%] [G loss: 1.164343]\n",
      "553 [D loss: 0.627311, acc.: 70.31%] [G loss: 1.213386]\n",
      "554 [D loss: 0.582301, acc.: 68.75%] [G loss: 1.338446]\n",
      "555 [D loss: 0.670287, acc.: 59.38%] [G loss: 1.263365]\n",
      "556 [D loss: 0.639762, acc.: 62.50%] [G loss: 1.360491]\n",
      "557 [D loss: 0.621904, acc.: 67.19%] [G loss: 1.357810]\n",
      "558 [D loss: 0.643517, acc.: 60.94%] [G loss: 1.329879]\n",
      "559 [D loss: 0.650109, acc.: 62.50%] [G loss: 1.377670]\n",
      "560 [D loss: 0.651125, acc.: 64.06%] [G loss: 1.384275]\n",
      "561 [D loss: 0.627779, acc.: 60.94%] [G loss: 1.295596]\n",
      "562 [D loss: 0.597016, acc.: 70.31%] [G loss: 1.555191]\n",
      "563 [D loss: 0.610259, acc.: 65.62%] [G loss: 1.434507]\n",
      "564 [D loss: 0.719834, acc.: 57.81%] [G loss: 1.392923]\n",
      "565 [D loss: 0.620258, acc.: 65.62%] [G loss: 1.359374]\n",
      "566 [D loss: 0.643328, acc.: 65.62%] [G loss: 1.351611]\n",
      "567 [D loss: 0.614158, acc.: 68.75%] [G loss: 1.351761]\n",
      "568 [D loss: 0.607457, acc.: 65.62%] [G loss: 1.480961]\n",
      "569 [D loss: 0.587526, acc.: 68.75%] [G loss: 1.419997]\n",
      "570 [D loss: 0.585142, acc.: 65.62%] [G loss: 1.410252]\n",
      "571 [D loss: 0.747856, acc.: 50.00%] [G loss: 1.135177]\n",
      "572 [D loss: 0.647929, acc.: 65.62%] [G loss: 1.156770]\n",
      "573 [D loss: 0.611618, acc.: 67.19%] [G loss: 1.203275]\n",
      "574 [D loss: 0.676823, acc.: 60.94%] [G loss: 1.290984]\n",
      "575 [D loss: 0.638155, acc.: 70.31%] [G loss: 1.284919]\n",
      "576 [D loss: 0.583882, acc.: 65.62%] [G loss: 1.442647]\n",
      "577 [D loss: 0.633850, acc.: 65.62%] [G loss: 1.505357]\n",
      "578 [D loss: 0.606190, acc.: 73.44%] [G loss: 1.551826]\n",
      "579 [D loss: 0.730698, acc.: 56.25%] [G loss: 1.321319]\n",
      "580 [D loss: 0.639940, acc.: 64.06%] [G loss: 1.243037]\n",
      "581 [D loss: 0.646400, acc.: 64.06%] [G loss: 1.259227]\n",
      "582 [D loss: 0.635415, acc.: 70.31%] [G loss: 1.256748]\n",
      "583 [D loss: 0.627725, acc.: 62.50%] [G loss: 1.278638]\n",
      "584 [D loss: 0.580116, acc.: 67.19%] [G loss: 1.375968]\n",
      "585 [D loss: 0.543960, acc.: 73.44%] [G loss: 1.521660]\n",
      "586 [D loss: 0.478308, acc.: 82.81%] [G loss: 1.620711]\n",
      "587 [D loss: 0.689808, acc.: 60.94%] [G loss: 1.275228]\n",
      "588 [D loss: 0.692018, acc.: 54.69%] [G loss: 1.215554]\n",
      "589 [D loss: 0.709024, acc.: 56.25%] [G loss: 1.214903]\n",
      "590 [D loss: 0.704649, acc.: 53.12%] [G loss: 1.064047]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "591 [D loss: 0.677461, acc.: 59.38%] [G loss: 1.124156]\n",
      "592 [D loss: 0.686002, acc.: 53.12%] [G loss: 1.114775]\n",
      "593 [D loss: 0.686075, acc.: 56.25%] [G loss: 1.067005]\n",
      "594 [D loss: 0.644937, acc.: 65.62%] [G loss: 1.106873]\n",
      "595 [D loss: 0.649610, acc.: 59.38%] [G loss: 1.471030]\n",
      "596 [D loss: 0.598183, acc.: 68.75%] [G loss: 1.812151]\n",
      "597 [D loss: 0.841888, acc.: 57.81%] [G loss: 0.948669]\n",
      "598 [D loss: 0.659562, acc.: 64.06%] [G loss: 0.945427]\n",
      "599 [D loss: 0.626052, acc.: 70.31%] [G loss: 0.992432]\n",
      "600 [D loss: 0.606044, acc.: 78.12%] [G loss: 1.104591]\n",
      "601 [D loss: 0.637479, acc.: 64.06%] [G loss: 1.176699]\n",
      "602 [D loss: 0.621053, acc.: 64.06%] [G loss: 1.308265]\n",
      "603 [D loss: 0.534505, acc.: 75.00%] [G loss: 1.325810]\n",
      "604 [D loss: 0.644160, acc.: 62.50%] [G loss: 1.204610]\n",
      "605 [D loss: 0.605093, acc.: 68.75%] [G loss: 1.329285]\n",
      "606 [D loss: 0.539110, acc.: 70.31%] [G loss: 1.577962]\n",
      "607 [D loss: 0.712817, acc.: 59.38%] [G loss: 1.233343]\n",
      "608 [D loss: 0.548163, acc.: 79.69%] [G loss: 1.217500]\n",
      "609 [D loss: 0.587406, acc.: 65.62%] [G loss: 1.345191]\n",
      "610 [D loss: 0.568257, acc.: 70.31%] [G loss: 1.372247]\n",
      "611 [D loss: 0.593279, acc.: 70.31%] [G loss: 1.297559]\n",
      "612 [D loss: 0.657727, acc.: 75.00%] [G loss: 1.223247]\n",
      "613 [D loss: 0.591941, acc.: 65.62%] [G loss: 1.160360]\n",
      "614 [D loss: 0.649659, acc.: 57.81%] [G loss: 1.163023]\n",
      "615 [D loss: 0.609827, acc.: 68.75%] [G loss: 1.315245]\n",
      "616 [D loss: 0.614660, acc.: 62.50%] [G loss: 1.294018]\n",
      "617 [D loss: 0.577462, acc.: 73.44%] [G loss: 1.273950]\n",
      "618 [D loss: 0.585094, acc.: 65.62%] [G loss: 1.197562]\n",
      "619 [D loss: 0.538607, acc.: 78.12%] [G loss: 1.250568]\n",
      "620 [D loss: 0.598194, acc.: 73.44%] [G loss: 1.227344]\n",
      "621 [D loss: 0.603044, acc.: 62.50%] [G loss: 1.234889]\n",
      "622 [D loss: 0.576839, acc.: 71.88%] [G loss: 1.362398]\n",
      "623 [D loss: 0.576949, acc.: 68.75%] [G loss: 1.461228]\n",
      "624 [D loss: 0.586240, acc.: 73.44%] [G loss: 1.445287]\n",
      "625 [D loss: 0.619583, acc.: 71.88%] [G loss: 1.415585]\n",
      "626 [D loss: 0.657882, acc.: 59.38%] [G loss: 1.286870]\n",
      "627 [D loss: 0.606573, acc.: 62.50%] [G loss: 1.276603]\n",
      "628 [D loss: 0.571907, acc.: 64.06%] [G loss: 1.348832]\n",
      "629 [D loss: 0.581532, acc.: 76.56%] [G loss: 1.376183]\n",
      "630 [D loss: 0.621892, acc.: 76.56%] [G loss: 1.485677]\n",
      "631 [D loss: 0.676747, acc.: 62.50%] [G loss: 1.316870]\n",
      "632 [D loss: 0.548908, acc.: 75.00%] [G loss: 1.253241]\n",
      "633 [D loss: 0.600107, acc.: 64.06%] [G loss: 1.256502]\n",
      "634 [D loss: 0.510710, acc.: 78.12%] [G loss: 1.457745]\n",
      "635 [D loss: 0.629303, acc.: 68.75%] [G loss: 1.242408]\n",
      "636 [D loss: 0.574630, acc.: 70.31%] [G loss: 1.190215]\n",
      "637 [D loss: 0.529222, acc.: 75.00%] [G loss: 1.472683]\n",
      "638 [D loss: 0.573499, acc.: 68.75%] [G loss: 1.510304]\n",
      "639 [D loss: 0.567920, acc.: 70.31%] [G loss: 1.584266]\n",
      "640 [D loss: 0.580494, acc.: 71.88%] [G loss: 1.389418]\n",
      "641 [D loss: 0.655852, acc.: 64.06%] [G loss: 1.404427]\n",
      "642 [D loss: 0.645076, acc.: 65.62%] [G loss: 1.395398]\n",
      "643 [D loss: 0.568495, acc.: 73.44%] [G loss: 1.478433]\n",
      "644 [D loss: 0.563957, acc.: 73.44%] [G loss: 1.662074]\n",
      "645 [D loss: 0.623153, acc.: 68.75%] [G loss: 1.558534]\n",
      "646 [D loss: 0.475544, acc.: 76.56%] [G loss: 1.468788]\n",
      "647 [D loss: 0.645915, acc.: 64.06%] [G loss: 1.259172]\n",
      "648 [D loss: 0.685059, acc.: 59.38%] [G loss: 1.214440]\n",
      "649 [D loss: 0.580322, acc.: 71.88%] [G loss: 1.237260]\n",
      "650 [D loss: 0.610682, acc.: 65.62%] [G loss: 1.345279]\n",
      "651 [D loss: 0.572569, acc.: 71.88%] [G loss: 1.400557]\n",
      "652 [D loss: 0.631488, acc.: 60.94%] [G loss: 1.348536]\n",
      "653 [D loss: 0.541253, acc.: 76.56%] [G loss: 1.253406]\n",
      "654 [D loss: 0.577085, acc.: 76.56%] [G loss: 1.366923]\n",
      "655 [D loss: 0.561204, acc.: 76.56%] [G loss: 1.364090]\n",
      "656 [D loss: 0.497988, acc.: 79.69%] [G loss: 1.347522]\n",
      "657 [D loss: 0.554292, acc.: 70.31%] [G loss: 1.428713]\n",
      "658 [D loss: 0.655092, acc.: 60.94%] [G loss: 1.198786]\n",
      "659 [D loss: 0.828868, acc.: 42.19%] [G loss: 1.054008]\n",
      "660 [D loss: 0.700895, acc.: 51.56%] [G loss: 1.128166]\n",
      "661 [D loss: 0.633479, acc.: 64.06%] [G loss: 1.123686]\n",
      "662 [D loss: 0.628241, acc.: 67.19%] [G loss: 1.177522]\n",
      "663 [D loss: 0.625600, acc.: 70.31%] [G loss: 1.138364]\n",
      "664 [D loss: 0.680875, acc.: 60.94%] [G loss: 1.210132]\n",
      "665 [D loss: 0.661644, acc.: 57.81%] [G loss: 1.147642]\n",
      "666 [D loss: 0.586989, acc.: 73.44%] [G loss: 1.161118]\n",
      "667 [D loss: 0.575329, acc.: 70.31%] [G loss: 1.099055]\n",
      "668 [D loss: 0.622546, acc.: 60.94%] [G loss: 1.176496]\n",
      "669 [D loss: 0.591076, acc.: 67.19%] [G loss: 1.169363]\n",
      "670 [D loss: 0.648086, acc.: 57.81%] [G loss: 1.042076]\n",
      "671 [D loss: 0.633977, acc.: 56.25%] [G loss: 1.095476]\n",
      "672 [D loss: 0.662636, acc.: 51.56%] [G loss: 1.142085]\n",
      "673 [D loss: 0.590661, acc.: 76.56%] [G loss: 1.066787]\n",
      "674 [D loss: 0.664051, acc.: 64.06%] [G loss: 1.122586]\n",
      "675 [D loss: 0.549887, acc.: 78.12%] [G loss: 1.174081]\n",
      "676 [D loss: 0.551047, acc.: 73.44%] [G loss: 1.311203]\n",
      "677 [D loss: 0.596720, acc.: 65.62%] [G loss: 1.303389]\n",
      "678 [D loss: 0.610294, acc.: 64.06%] [G loss: 1.659998]\n",
      "679 [D loss: 0.673836, acc.: 64.06%] [G loss: 1.573013]\n",
      "680 [D loss: 0.812242, acc.: 46.88%] [G loss: 1.108760]\n",
      "681 [D loss: 0.718016, acc.: 51.56%] [G loss: 1.038986]\n",
      "682 [D loss: 0.649004, acc.: 62.50%] [G loss: 1.015525]\n",
      "683 [D loss: 0.667040, acc.: 51.56%] [G loss: 0.911373]\n",
      "684 [D loss: 0.557680, acc.: 75.00%] [G loss: 0.986169]\n",
      "685 [D loss: 0.537996, acc.: 78.12%] [G loss: 1.093970]\n",
      "686 [D loss: 0.539281, acc.: 70.31%] [G loss: 1.163039]\n",
      "687 [D loss: 0.562214, acc.: 73.44%] [G loss: 1.150595]\n",
      "688 [D loss: 0.531002, acc.: 73.44%] [G loss: 1.079920]\n",
      "689 [D loss: 0.650821, acc.: 67.19%] [G loss: 1.041197]\n",
      "690 [D loss: 0.523299, acc.: 73.44%] [G loss: 1.100763]\n",
      "691 [D loss: 0.686229, acc.: 73.44%] [G loss: 1.079052]\n",
      "692 [D loss: 0.506240, acc.: 75.00%] [G loss: 1.113070]\n",
      "693 [D loss: 0.609193, acc.: 67.19%] [G loss: 1.130044]\n",
      "694 [D loss: 0.476454, acc.: 84.38%] [G loss: 1.189151]\n",
      "695 [D loss: 0.548495, acc.: 76.56%] [G loss: 1.230635]\n",
      "696 [D loss: 0.626475, acc.: 67.19%] [G loss: 1.201856]\n",
      "697 [D loss: 0.592640, acc.: 71.88%] [G loss: 1.197992]\n",
      "698 [D loss: 0.533720, acc.: 70.31%] [G loss: 1.213087]\n",
      "699 [D loss: 0.464227, acc.: 81.25%] [G loss: 1.301556]\n",
      "700 [D loss: 0.559140, acc.: 73.44%] [G loss: 1.352110]\n",
      "701 [D loss: 0.626935, acc.: 60.94%] [G loss: 1.257935]\n",
      "702 [D loss: 0.520803, acc.: 79.69%] [G loss: 1.349881]\n",
      "703 [D loss: 0.563016, acc.: 70.31%] [G loss: 1.214824]\n",
      "704 [D loss: 0.523301, acc.: 78.12%] [G loss: 1.284747]\n",
      "705 [D loss: 0.456904, acc.: 78.12%] [G loss: 1.360969]\n",
      "706 [D loss: 0.729270, acc.: 64.06%] [G loss: 1.317192]\n",
      "707 [D loss: 0.591823, acc.: 68.75%] [G loss: 1.257191]\n",
      "708 [D loss: 0.589348, acc.: 73.44%] [G loss: 1.264910]\n",
      "709 [D loss: 0.474716, acc.: 82.81%] [G loss: 1.311448]\n",
      "710 [D loss: 0.497896, acc.: 78.12%] [G loss: 1.410584]\n",
      "711 [D loss: 0.615512, acc.: 76.56%] [G loss: 1.325072]\n",
      "712 [D loss: 0.505094, acc.: 79.69%] [G loss: 1.381909]\n",
      "713 [D loss: 0.593332, acc.: 70.31%] [G loss: 1.220277]\n",
      "714 [D loss: 0.640148, acc.: 70.31%] [G loss: 1.331712]\n",
      "715 [D loss: 0.562105, acc.: 68.75%] [G loss: 1.312051]\n",
      "716 [D loss: 0.503021, acc.: 76.56%] [G loss: 1.398367]\n",
      "717 [D loss: 0.645366, acc.: 64.06%] [G loss: 1.334870]\n",
      "718 [D loss: 0.519946, acc.: 79.69%] [G loss: 1.269396]\n",
      "719 [D loss: 0.514441, acc.: 70.31%] [G loss: 1.305267]\n",
      "720 [D loss: 0.469548, acc.: 76.56%] [G loss: 1.348862]\n",
      "721 [D loss: 0.503155, acc.: 76.56%] [G loss: 1.477775]\n",
      "722 [D loss: 0.453433, acc.: 81.25%] [G loss: 1.585253]\n",
      "723 [D loss: 0.576998, acc.: 68.75%] [G loss: 1.633740]\n",
      "724 [D loss: 0.563118, acc.: 67.19%] [G loss: 1.489197]\n",
      "725 [D loss: 0.585374, acc.: 68.75%] [G loss: 1.329827]\n",
      "726 [D loss: 0.420533, acc.: 82.81%] [G loss: 1.375861]\n",
      "727 [D loss: 0.667717, acc.: 68.75%] [G loss: 1.359536]\n",
      "728 [D loss: 0.396714, acc.: 82.81%] [G loss: 1.368034]\n",
      "729 [D loss: 0.599149, acc.: 67.19%] [G loss: 1.345703]\n",
      "730 [D loss: 0.445766, acc.: 81.25%] [G loss: 1.315431]\n",
      "731 [D loss: 0.543692, acc.: 71.88%] [G loss: 1.318227]\n",
      "732 [D loss: 0.622296, acc.: 62.50%] [G loss: 1.443537]\n",
      "733 [D loss: 0.537423, acc.: 73.44%] [G loss: 1.402926]\n",
      "734 [D loss: 0.490836, acc.: 78.12%] [G loss: 1.356233]\n",
      "735 [D loss: 0.472353, acc.: 79.69%] [G loss: 1.472254]\n",
      "736 [D loss: 0.576602, acc.: 75.00%] [G loss: 1.330775]\n",
      "737 [D loss: 0.474320, acc.: 79.69%] [G loss: 1.403216]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "738 [D loss: 0.404375, acc.: 85.94%] [G loss: 1.491548]\n",
      "739 [D loss: 0.550938, acc.: 73.44%] [G loss: 1.321597]\n",
      "740 [D loss: 0.459841, acc.: 79.69%] [G loss: 1.434003]\n",
      "741 [D loss: 0.363920, acc.: 84.38%] [G loss: 1.633643]\n",
      "742 [D loss: 0.617954, acc.: 67.19%] [G loss: 1.530069]\n",
      "743 [D loss: 0.573350, acc.: 70.31%] [G loss: 1.405737]\n",
      "744 [D loss: 0.598215, acc.: 64.06%] [G loss: 1.278362]\n",
      "745 [D loss: 0.483584, acc.: 79.69%] [G loss: 1.263528]\n",
      "746 [D loss: 0.552810, acc.: 75.00%] [G loss: 1.283184]\n",
      "747 [D loss: 0.570038, acc.: 67.19%] [G loss: 1.248399]\n",
      "748 [D loss: 0.517658, acc.: 75.00%] [G loss: 1.233825]\n",
      "749 [D loss: 0.613380, acc.: 68.75%] [G loss: 1.255625]\n",
      "750 [D loss: 0.605952, acc.: 67.19%] [G loss: 1.149690]\n",
      "751 [D loss: 0.730547, acc.: 56.25%] [G loss: 1.101903]\n",
      "752 [D loss: 0.580534, acc.: 68.75%] [G loss: 1.182458]\n",
      "753 [D loss: 0.569519, acc.: 75.00%] [G loss: 1.241552]\n",
      "754 [D loss: 0.556611, acc.: 71.88%] [G loss: 1.249733]\n",
      "755 [D loss: 0.464693, acc.: 79.69%] [G loss: 1.307983]\n",
      "756 [D loss: 0.611646, acc.: 68.75%] [G loss: 1.310161]\n",
      "757 [D loss: 0.368274, acc.: 90.62%] [G loss: 1.383898]\n",
      "758 [D loss: 0.589216, acc.: 71.88%] [G loss: 1.266015]\n",
      "759 [D loss: 0.634368, acc.: 62.50%] [G loss: 1.278259]\n",
      "760 [D loss: 0.524810, acc.: 75.00%] [G loss: 1.384402]\n",
      "761 [D loss: 0.524551, acc.: 73.44%] [G loss: 1.450896]\n",
      "762 [D loss: 0.402980, acc.: 82.81%] [G loss: 1.563098]\n",
      "763 [D loss: 0.605336, acc.: 75.00%] [G loss: 1.601026]\n",
      "764 [D loss: 0.442907, acc.: 79.69%] [G loss: 1.588742]\n",
      "765 [D loss: 0.571137, acc.: 71.88%] [G loss: 1.455331]\n",
      "766 [D loss: 0.580320, acc.: 59.38%] [G loss: 3.871269]\n",
      "767 [D loss: 2.003853, acc.: 1.56%] [G loss: 1.592241]\n",
      "768 [D loss: 0.570909, acc.: 71.88%] [G loss: 1.609653]\n",
      "769 [D loss: 0.625085, acc.: 76.56%] [G loss: 1.532980]\n",
      "770 [D loss: 0.518385, acc.: 75.00%] [G loss: 1.378216]\n",
      "771 [D loss: 0.489744, acc.: 82.81%] [G loss: 1.396405]\n",
      "772 [D loss: 0.564453, acc.: 76.56%] [G loss: 1.355926]\n",
      "773 [D loss: 0.352837, acc.: 89.06%] [G loss: 1.485790]\n",
      "774 [D loss: 0.686349, acc.: 67.19%] [G loss: 1.418576]\n",
      "775 [D loss: 0.624972, acc.: 67.19%] [G loss: 1.230477]\n",
      "776 [D loss: 0.647200, acc.: 64.06%] [G loss: 1.161355]\n",
      "777 [D loss: 0.629017, acc.: 68.75%] [G loss: 1.084867]\n",
      "778 [D loss: 0.568741, acc.: 73.44%] [G loss: 1.072994]\n",
      "779 [D loss: 0.590231, acc.: 71.88%] [G loss: 1.117613]\n",
      "780 [D loss: 0.499515, acc.: 81.25%] [G loss: 1.128639]\n",
      "781 [D loss: 0.509726, acc.: 73.44%] [G loss: 1.262691]\n",
      "782 [D loss: 0.499441, acc.: 73.44%] [G loss: 1.339270]\n",
      "783 [D loss: 0.421531, acc.: 84.38%] [G loss: 1.518543]\n",
      "784 [D loss: 0.497168, acc.: 79.69%] [G loss: 1.567421]\n",
      "785 [D loss: 0.581380, acc.: 68.75%] [G loss: 1.423015]\n",
      "786 [D loss: 0.550181, acc.: 71.88%] [G loss: 1.323650]\n",
      "787 [D loss: 0.433884, acc.: 79.69%] [G loss: 1.455932]\n",
      "788 [D loss: 0.490878, acc.: 76.56%] [G loss: 1.524812]\n",
      "789 [D loss: 0.672641, acc.: 70.31%] [G loss: 1.449066]\n",
      "790 [D loss: 0.658225, acc.: 60.94%] [G loss: 1.339306]\n",
      "791 [D loss: 0.396620, acc.: 85.94%] [G loss: 1.541102]\n",
      "792 [D loss: 0.727963, acc.: 57.81%] [G loss: 1.400521]\n",
      "793 [D loss: 0.464757, acc.: 79.69%] [G loss: 1.487869]\n",
      "794 [D loss: 0.452276, acc.: 81.25%] [G loss: 1.547723]\n",
      "795 [D loss: 0.459028, acc.: 76.56%] [G loss: 1.654718]\n",
      "796 [D loss: 0.460587, acc.: 78.12%] [G loss: 1.597085]\n",
      "797 [D loss: 0.463326, acc.: 78.12%] [G loss: 1.542094]\n",
      "798 [D loss: 0.442724, acc.: 79.69%] [G loss: 1.542747]\n",
      "799 [D loss: 0.531400, acc.: 76.56%] [G loss: 1.526199]\n",
      "800 [D loss: 0.510335, acc.: 76.56%] [G loss: 1.444870]\n",
      "801 [D loss: 0.396279, acc.: 82.81%] [G loss: 1.604473]\n",
      "802 [D loss: 0.544988, acc.: 73.44%] [G loss: 1.617115]\n",
      "803 [D loss: 0.518207, acc.: 75.00%] [G loss: 1.490106]\n",
      "804 [D loss: 0.568516, acc.: 75.00%] [G loss: 1.396160]\n",
      "805 [D loss: 0.582785, acc.: 71.88%] [G loss: 1.414513]\n",
      "806 [D loss: 0.557332, acc.: 68.75%] [G loss: 1.340197]\n",
      "807 [D loss: 0.449710, acc.: 82.81%] [G loss: 1.413522]\n",
      "808 [D loss: 0.649697, acc.: 65.62%] [G loss: 1.289664]\n",
      "809 [D loss: 0.423489, acc.: 82.81%] [G loss: 1.419419]\n",
      "810 [D loss: 0.461983, acc.: 78.12%] [G loss: 1.488611]\n",
      "811 [D loss: 0.517305, acc.: 75.00%] [G loss: 1.491309]\n",
      "812 [D loss: 0.602406, acc.: 65.62%] [G loss: 1.390133]\n",
      "813 [D loss: 0.485724, acc.: 79.69%] [G loss: 1.367371]\n",
      "814 [D loss: 0.527438, acc.: 73.44%] [G loss: 1.447455]\n",
      "815 [D loss: 0.563547, acc.: 70.31%] [G loss: 1.423145]\n",
      "816 [D loss: 0.581965, acc.: 75.00%] [G loss: 1.425040]\n",
      "817 [D loss: 0.443353, acc.: 84.38%] [G loss: 1.474703]\n",
      "818 [D loss: 0.503211, acc.: 78.12%] [G loss: 1.455872]\n",
      "819 [D loss: 0.567215, acc.: 68.75%] [G loss: 1.320377]\n",
      "820 [D loss: 0.494602, acc.: 79.69%] [G loss: 1.341943]\n",
      "821 [D loss: 0.520498, acc.: 78.12%] [G loss: 1.387661]\n",
      "822 [D loss: 0.490753, acc.: 76.56%] [G loss: 1.460328]\n",
      "823 [D loss: 0.395152, acc.: 84.38%] [G loss: 1.548021]\n",
      "824 [D loss: 0.473808, acc.: 76.56%] [G loss: 1.582363]\n",
      "825 [D loss: 0.615444, acc.: 67.19%] [G loss: 1.555461]\n",
      "826 [D loss: 0.524551, acc.: 79.69%] [G loss: 1.469114]\n",
      "827 [D loss: 0.497919, acc.: 75.00%] [G loss: 1.454400]\n",
      "828 [D loss: 0.604259, acc.: 67.19%] [G loss: 1.348240]\n",
      "829 [D loss: 0.550823, acc.: 71.88%] [G loss: 1.317524]\n",
      "830 [D loss: 0.488207, acc.: 81.25%] [G loss: 1.424212]\n",
      "831 [D loss: 0.504855, acc.: 75.00%] [G loss: 1.442856]\n",
      "832 [D loss: 0.386488, acc.: 85.94%] [G loss: 1.582191]\n",
      "833 [D loss: 0.485482, acc.: 75.00%] [G loss: 1.659904]\n",
      "834 [D loss: 0.480022, acc.: 81.25%] [G loss: 1.613415]\n",
      "835 [D loss: 0.539882, acc.: 76.56%] [G loss: 1.505981]\n",
      "836 [D loss: 0.515602, acc.: 76.56%] [G loss: 1.450099]\n",
      "837 [D loss: 0.712605, acc.: 67.19%] [G loss: 1.430616]\n",
      "838 [D loss: 0.535887, acc.: 73.44%] [G loss: 1.335529]\n",
      "839 [D loss: 0.382726, acc.: 85.94%] [G loss: 1.448676]\n",
      "840 [D loss: 0.651043, acc.: 68.75%] [G loss: 1.322782]\n",
      "841 [D loss: 0.630206, acc.: 70.31%] [G loss: 1.347126]\n",
      "842 [D loss: 0.413718, acc.: 81.25%] [G loss: 1.513821]\n",
      "843 [D loss: 0.429987, acc.: 78.12%] [G loss: 1.593898]\n",
      "844 [D loss: 0.482657, acc.: 76.56%] [G loss: 1.567344]\n",
      "845 [D loss: 0.503223, acc.: 76.56%] [G loss: 1.603971]\n",
      "846 [D loss: 0.471318, acc.: 78.12%] [G loss: 1.653155]\n",
      "847 [D loss: 0.365733, acc.: 85.94%] [G loss: 1.760896]\n",
      "848 [D loss: 0.511731, acc.: 73.44%] [G loss: 1.698043]\n",
      "849 [D loss: 0.534980, acc.: 73.44%] [G loss: 1.471396]\n",
      "850 [D loss: 0.511972, acc.: 79.69%] [G loss: 1.431798]\n",
      "851 [D loss: 0.526792, acc.: 79.69%] [G loss: 1.638764]\n",
      "852 [D loss: 0.501478, acc.: 75.00%] [G loss: 1.582518]\n",
      "853 [D loss: 0.497541, acc.: 78.12%] [G loss: 1.637109]\n",
      "854 [D loss: 0.575502, acc.: 70.31%] [G loss: 1.629268]\n",
      "855 [D loss: 0.459972, acc.: 81.25%] [G loss: 1.660734]\n",
      "856 [D loss: 0.492158, acc.: 76.56%] [G loss: 1.581930]\n",
      "857 [D loss: 0.573509, acc.: 71.88%] [G loss: 1.451144]\n",
      "858 [D loss: 0.513868, acc.: 76.56%] [G loss: 1.532239]\n",
      "859 [D loss: 0.462668, acc.: 79.69%] [G loss: 1.643064]\n",
      "860 [D loss: 0.416894, acc.: 84.38%] [G loss: 1.607801]\n",
      "861 [D loss: 0.537729, acc.: 78.12%] [G loss: 1.625117]\n",
      "862 [D loss: 0.430832, acc.: 81.25%] [G loss: 1.637171]\n",
      "863 [D loss: 0.542269, acc.: 73.44%] [G loss: 1.662327]\n",
      "864 [D loss: 0.555685, acc.: 70.31%] [G loss: 1.559858]\n",
      "865 [D loss: 0.480730, acc.: 81.25%] [G loss: 1.600566]\n",
      "866 [D loss: 0.492222, acc.: 76.56%] [G loss: 1.602643]\n",
      "867 [D loss: 0.435872, acc.: 82.81%] [G loss: 1.703685]\n",
      "868 [D loss: 0.562257, acc.: 71.88%] [G loss: 1.603349]\n",
      "869 [D loss: 0.485362, acc.: 79.69%] [G loss: 1.556563]\n",
      "870 [D loss: 0.507699, acc.: 76.56%] [G loss: 1.471730]\n",
      "871 [D loss: 0.473233, acc.: 78.12%] [G loss: 1.460649]\n",
      "872 [D loss: 0.310851, acc.: 92.19%] [G loss: 1.570961]\n",
      "873 [D loss: 0.461844, acc.: 79.69%] [G loss: 1.670774]\n",
      "874 [D loss: 0.434880, acc.: 81.25%] [G loss: 1.682878]\n",
      "875 [D loss: 0.364808, acc.: 85.94%] [G loss: 1.750263]\n",
      "876 [D loss: 0.545689, acc.: 70.31%] [G loss: 1.703322]\n",
      "877 [D loss: 0.319919, acc.: 89.06%] [G loss: 1.829943]\n",
      "878 [D loss: 0.572158, acc.: 76.56%] [G loss: 1.667059]\n",
      "879 [D loss: 0.570832, acc.: 73.44%] [G loss: 1.486586]\n",
      "880 [D loss: 0.569752, acc.: 68.75%] [G loss: 1.474914]\n",
      "881 [D loss: 0.529174, acc.: 76.56%] [G loss: 1.619165]\n",
      "882 [D loss: 0.513371, acc.: 78.12%] [G loss: 1.690454]\n",
      "883 [D loss: 0.507187, acc.: 78.12%] [G loss: 1.709627]\n",
      "884 [D loss: 0.522896, acc.: 73.44%] [G loss: 1.738876]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "885 [D loss: 0.388307, acc.: 84.38%] [G loss: 1.786777]\n",
      "886 [D loss: 0.537724, acc.: 75.00%] [G loss: 1.658026]\n",
      "887 [D loss: 0.437240, acc.: 78.12%] [G loss: 1.725830]\n",
      "888 [D loss: 0.542592, acc.: 76.56%] [G loss: 1.640850]\n",
      "889 [D loss: 0.619803, acc.: 62.50%] [G loss: 1.469436]\n",
      "890 [D loss: 0.480707, acc.: 79.69%] [G loss: 1.429758]\n",
      "891 [D loss: 0.400478, acc.: 85.94%] [G loss: 1.711182]\n",
      "892 [D loss: 0.470243, acc.: 78.12%] [G loss: 1.733736]\n",
      "893 [D loss: 0.551733, acc.: 70.31%] [G loss: 1.568646]\n",
      "894 [D loss: 0.496389, acc.: 78.12%] [G loss: 1.497298]\n",
      "895 [D loss: 0.496450, acc.: 73.44%] [G loss: 1.513441]\n",
      "896 [D loss: 0.377612, acc.: 84.38%] [G loss: 1.654515]\n",
      "897 [D loss: 0.794598, acc.: 65.62%] [G loss: 1.415882]\n",
      "898 [D loss: 0.442989, acc.: 78.12%] [G loss: 1.358044]\n",
      "899 [D loss: 0.525957, acc.: 73.44%] [G loss: 1.350713]\n",
      "900 [D loss: 0.502764, acc.: 71.88%] [G loss: 1.411953]\n",
      "901 [D loss: 0.509929, acc.: 75.00%] [G loss: 1.439884]\n",
      "902 [D loss: 0.562738, acc.: 68.75%] [G loss: 1.347635]\n",
      "903 [D loss: 0.563569, acc.: 68.75%] [G loss: 1.310510]\n",
      "904 [D loss: 0.576673, acc.: 65.62%] [G loss: 1.359339]\n",
      "905 [D loss: 0.577338, acc.: 65.62%] [G loss: 1.353999]\n",
      "906 [D loss: 0.535184, acc.: 67.19%] [G loss: 1.247129]\n",
      "907 [D loss: 0.639955, acc.: 59.38%] [G loss: 1.243462]\n",
      "908 [D loss: 0.540303, acc.: 76.56%] [G loss: 1.316565]\n",
      "909 [D loss: 0.528134, acc.: 73.44%] [G loss: 1.311775]\n",
      "910 [D loss: 0.487283, acc.: 79.69%] [G loss: 1.346540]\n",
      "911 [D loss: 0.470510, acc.: 79.69%] [G loss: 1.490074]\n",
      "912 [D loss: 0.461059, acc.: 73.44%] [G loss: 1.496298]\n",
      "913 [D loss: 0.603002, acc.: 70.31%] [G loss: 1.415143]\n",
      "914 [D loss: 0.594785, acc.: 70.31%] [G loss: 1.385523]\n",
      "915 [D loss: 0.545861, acc.: 70.31%] [G loss: 1.366310]\n",
      "916 [D loss: 0.459366, acc.: 78.12%] [G loss: 1.372283]\n",
      "917 [D loss: 0.491073, acc.: 75.00%] [G loss: 1.464781]\n",
      "918 [D loss: 0.503334, acc.: 79.69%] [G loss: 1.540854]\n",
      "919 [D loss: 0.435353, acc.: 84.38%] [G loss: 1.594941]\n",
      "920 [D loss: 0.391241, acc.: 85.94%] [G loss: 1.721830]\n",
      "921 [D loss: 0.534182, acc.: 75.00%] [G loss: 1.622507]\n",
      "922 [D loss: 0.488812, acc.: 82.81%] [G loss: 1.696368]\n",
      "923 [D loss: 0.520082, acc.: 76.56%] [G loss: 1.716551]\n",
      "924 [D loss: 0.350367, acc.: 89.06%] [G loss: 2.132603]\n",
      "925 [D loss: 0.564212, acc.: 73.44%] [G loss: 1.803663]\n",
      "926 [D loss: 0.423490, acc.: 81.25%] [G loss: 2.038551]\n",
      "927 [D loss: 0.425603, acc.: 79.69%] [G loss: 2.272339]\n",
      "928 [D loss: 0.543802, acc.: 78.12%] [G loss: 1.951872]\n",
      "929 [D loss: 0.581825, acc.: 73.44%] [G loss: 1.962990]\n",
      "930 [D loss: 0.371307, acc.: 85.94%] [G loss: 1.970281]\n",
      "931 [D loss: 0.560419, acc.: 81.25%] [G loss: 1.962237]\n",
      "932 [D loss: 0.524061, acc.: 78.12%] [G loss: 1.902237]\n",
      "933 [D loss: 0.367535, acc.: 82.81%] [G loss: 1.831598]\n",
      "934 [D loss: 0.498075, acc.: 73.44%] [G loss: 1.840376]\n",
      "935 [D loss: 0.420057, acc.: 85.94%] [G loss: 2.024421]\n",
      "936 [D loss: 0.540117, acc.: 73.44%] [G loss: 2.000400]\n",
      "937 [D loss: 0.452262, acc.: 82.81%] [G loss: 1.820538]\n",
      "938 [D loss: 0.539870, acc.: 73.44%] [G loss: 1.753856]\n",
      "939 [D loss: 0.501705, acc.: 75.00%] [G loss: 1.634907]\n",
      "940 [D loss: 0.383003, acc.: 85.94%] [G loss: 1.694519]\n",
      "941 [D loss: 0.536130, acc.: 76.56%] [G loss: 1.682457]\n",
      "942 [D loss: 0.566993, acc.: 70.31%] [G loss: 1.527275]\n",
      "943 [D loss: 0.585215, acc.: 65.62%] [G loss: 1.539458]\n",
      "944 [D loss: 0.477435, acc.: 76.56%] [G loss: 1.657004]\n",
      "945 [D loss: 0.492510, acc.: 75.00%] [G loss: 1.683645]\n",
      "946 [D loss: 0.444484, acc.: 84.38%] [G loss: 1.689372]\n",
      "947 [D loss: 0.336786, acc.: 84.38%] [G loss: 1.940532]\n",
      "948 [D loss: 0.666752, acc.: 67.19%] [G loss: 1.794347]\n",
      "949 [D loss: 0.372480, acc.: 84.38%] [G loss: 1.686672]\n",
      "950 [D loss: 0.506282, acc.: 78.12%] [G loss: 1.742419]\n",
      "951 [D loss: 0.352283, acc.: 89.06%] [G loss: 1.916628]\n",
      "952 [D loss: 0.322312, acc.: 90.62%] [G loss: 2.060434]\n",
      "953 [D loss: 0.388237, acc.: 84.38%] [G loss: 2.027685]\n",
      "954 [D loss: 0.408179, acc.: 84.38%] [G loss: 1.969615]\n",
      "955 [D loss: 0.480325, acc.: 81.25%] [G loss: 1.844251]\n",
      "956 [D loss: 0.333717, acc.: 87.50%] [G loss: 1.922736]\n",
      "957 [D loss: 0.334742, acc.: 87.50%] [G loss: 2.036062]\n",
      "958 [D loss: 0.428204, acc.: 81.25%] [G loss: 1.945172]\n",
      "959 [D loss: 0.431240, acc.: 78.12%] [G loss: 1.786510]\n",
      "960 [D loss: 0.444432, acc.: 78.12%] [G loss: 1.887677]\n",
      "961 [D loss: 0.550121, acc.: 75.00%] [G loss: 1.951892]\n",
      "962 [D loss: 0.423112, acc.: 81.25%] [G loss: 1.935013]\n",
      "963 [D loss: 0.532405, acc.: 71.88%] [G loss: 1.929321]\n",
      "964 [D loss: 0.325529, acc.: 87.50%] [G loss: 1.993379]\n",
      "965 [D loss: 0.413995, acc.: 84.38%] [G loss: 1.991259]\n",
      "966 [D loss: 0.443078, acc.: 78.12%] [G loss: 1.985734]\n",
      "967 [D loss: 0.637210, acc.: 73.44%] [G loss: 1.953726]\n",
      "968 [D loss: 0.507722, acc.: 75.00%] [G loss: 1.784152]\n",
      "969 [D loss: 0.389721, acc.: 87.50%] [G loss: 1.849084]\n",
      "970 [D loss: 0.424922, acc.: 81.25%] [G loss: 1.834106]\n",
      "971 [D loss: 0.633922, acc.: 64.06%] [G loss: 1.613614]\n",
      "972 [D loss: 0.384317, acc.: 87.50%] [G loss: 1.632411]\n",
      "973 [D loss: 0.402047, acc.: 82.81%] [G loss: 1.788556]\n",
      "974 [D loss: 0.326822, acc.: 87.50%] [G loss: 1.876379]\n",
      "975 [D loss: 0.427900, acc.: 82.81%] [G loss: 1.919900]\n",
      "976 [D loss: 0.469396, acc.: 79.69%] [G loss: 1.770666]\n",
      "977 [D loss: 0.472211, acc.: 79.69%] [G loss: 1.948185]\n",
      "978 [D loss: 0.534968, acc.: 71.88%] [G loss: 1.835211]\n",
      "979 [D loss: 0.417587, acc.: 84.38%] [G loss: 1.989129]\n",
      "980 [D loss: 0.504908, acc.: 78.12%] [G loss: 1.908917]\n",
      "981 [D loss: 0.384915, acc.: 85.94%] [G loss: 1.958339]\n",
      "982 [D loss: 0.465480, acc.: 78.12%] [G loss: 1.888752]\n",
      "983 [D loss: 0.482191, acc.: 78.12%] [G loss: 1.938098]\n",
      "984 [D loss: 0.489290, acc.: 75.00%] [G loss: 1.934823]\n",
      "985 [D loss: 0.544553, acc.: 78.12%] [G loss: 1.854053]\n",
      "986 [D loss: 0.444307, acc.: 76.56%] [G loss: 1.882885]\n",
      "987 [D loss: 0.441056, acc.: 78.12%] [G loss: 1.986312]\n",
      "988 [D loss: 0.433005, acc.: 78.12%] [G loss: 1.913287]\n",
      "989 [D loss: 0.482921, acc.: 75.00%] [G loss: 1.903097]\n",
      "990 [D loss: 0.468761, acc.: 76.56%] [G loss: 1.905093]\n",
      "991 [D loss: 0.296657, acc.: 90.62%] [G loss: 2.124743]\n",
      "992 [D loss: 0.504494, acc.: 78.12%] [G loss: 2.235843]\n",
      "993 [D loss: 0.527053, acc.: 76.56%] [G loss: 1.981385]\n",
      "994 [D loss: 0.390014, acc.: 84.38%] [G loss: 1.937938]\n",
      "995 [D loss: 0.498228, acc.: 75.00%] [G loss: 1.885115]\n",
      "996 [D loss: 0.388213, acc.: 79.69%] [G loss: 1.924680]\n",
      "997 [D loss: 0.537230, acc.: 73.44%] [G loss: 1.836904]\n",
      "998 [D loss: 0.341854, acc.: 87.50%] [G loss: 1.858925]\n",
      "999 [D loss: 0.488012, acc.: 73.44%] [G loss: 1.901922]\n"
     ]
    }
   ],
   "source": [
    "gan = GAN(rows=100)\n",
    "gan.train(genre_dataset=r_list, epochs=1000, batch_size=32, sample_interval=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[58, 53, 46, 49, 49, 50, 44, 57, 47, 36, 59, 49, 51, 40, 59, 40, 38, 52, 48, 38, 52, 52, 48, 42, 52, 52, 52, 48, 55, 43, 38, 51, 55, 44, 43, 42, 50, 59, 46, 53, 47, 37, 59, 40, 45, 50, 57, 48, 40, 51, 38, 50, 49, 43, 36, 59, 37, 37, 59, 45, 51, 37, 49, 59, 36, 49, 53, 37, 59, 36, 55, 40, 37, 42, 59, 59, 51, 58, 36, 37, 36, 59, 58, 38, 42, 47, 36, 59, 37, 57, 57, 51, 36, 36, 38, 36, 55, 47, 47, 38]\n"
     ]
    }
   ],
   "source": [
    "def generate(gan, input_notes):\n",
    "        # Get pitch names and store in a dictionary\n",
    "        notes = input_notes\n",
    "        \n",
    "        pitchnames = sorted(set(item for item in notes))\n",
    "        int_to_note = dict((number, note) for number, note in enumerate(pitchnames))\n",
    "        #print (int_to_note)\n",
    "        # Use random noise to generate sequences\n",
    "        noise = np.random.normal(0, 1, (1, gan.latent_dim))\n",
    "        length = len(pitchnames) / 2\n",
    "        predictions = gan.generator.predict(noise)\n",
    "        #print(predictions)\n",
    "        pred_notes = [x*length+length for x in predictions[0]]\n",
    "        pred_notes = [int_to_note[int(x)] for x in pred_notes]\n",
    "        notess = []\n",
    "        #print(len(pred_notes))\n",
    "        for x in pred_notes:\n",
    "            notess.append(int(x))\n",
    "        return notess\n",
    "predictions = generate(gan, notes)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_midi(notes):\n",
    "    new_midi_data = pretty_midi.PrettyMIDI()\n",
    "    drum = pretty_midi.Instrument(program=0, is_drum=True, name=\"Midi Drums\" )\n",
    "    time = 0\n",
    "    step = 0.25\n",
    "    len_notes = len(notes)\n",
    "    vec_arr = np.random.uniform(50,125,len_notes)\n",
    "    delta_arr = np.random.uniform(0.2,0.5,len_notes)\n",
    "    for i,note_number in enumerate(notes):\n",
    "        myNote = pretty_midi.Note(velocity=int(vec_arr[i]), pitch=int(note_number), start=time, end=time+delta_arr[i])\n",
    "        drum.notes.append(myNote)\n",
    "        time += step\n",
    "    new_midi_data.instruments.append(drum)\n",
    "    return new_midi_data\n",
    "\n",
    "new_midi_data = create_midi(predictions)\n",
    "new_midi_data.write('/home/mark/repos/Springboard/result/final_2.mid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tested with full list of drum tracks\n",
    "full_list = groove_df.midi_filename.tolist()\n",
    "gan = GAN(rows=100)\n",
    "gan.train(genre_dataset=full_list, epochs=1000, batch_size=32, sample_interval=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = generate(gan, notes)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_midi_data = create_midi(predictions)\n",
    "new_midi_data.write('/home/mark/repos/Springboard/result/final_2.mid')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
